import os
import sys
import smtpd
import warnings
import matplotlib.pyplot as plt
import pandas as pd
warnings.filterwarnings('ignore')
import scipy.stats.morestats
import matplotlib
from scipy.ndimage import gaussian_filter, gaussian_filter1d
from sklearn.model_selection import KFold
from sklearn.naive_bayes import GaussianNB
from sklearn.manifold import MDS
import torch
from torch.nn import AvgPool1d
import seaborn as sns
sns.set(style='dark', font_scale=1.25)
from scipy.stats import *
from copy import deepcopy
from glob import glob
import scipy
from mat73 import loadmat
import joblib
from tqdm import tqdm
import numpy as np
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.proportion import proportions_chisquare as chisquare
from scipy.stats import ttest_1samp
from scipy.stats import sem
import matplotlib.path as mpath
import matplotlib.patches as patches
from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.base import BaseEstimator
from sklearn.metrics import euclidean_distances
from sklearn.utils import check_random_state, check_array, check_symmetric
from sklearn.isotonic import IsotonicRegression
from sklearn.utils.validation import _deprecate_positional_args
from joblib import Parallel, delayed, effective_n_jobs
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.naive_bayes import CategoricalNB
from scipy.spatial.distance import hamming
import ratinabox
from ratinabox.Environment import Environment
from ratinabox.Agent import Agent
from ratinabox.Neurons import *
from glob import glob
import os, warnings
import numpy as np
import pandas as pd
warnings.filterwarnings('ignore')
import scipy.stats.morestats
import matplotlib
import time
from scipy.ndimage import gaussian_filter
import torch
from matplotlib.colors import Normalize


def load_dat(animal, p, to_convert=["envs", "position", "trace"], format="joblib"):
    """
    Load dataset from target animal (matlab file), convert environment labels, position, and trace data
    to numpy arrays. Can load original MATLAB files, or saved joblib file with converted fields
    :param animal: animal ID (e.g., "QLAK-CA1-08")
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param to_convert: fields to convert to numpy arrays
    :param format: format of data file to be loaded, either "MATLAB" or "joblib"
    :return: dataset in nested dictionary with animal ID as first key
    """
    # Define path to datasets (original should be added to "data" folder after downloading)
    p_data = os.path.join(p, "data")
    print(f'Loading preprocessed data for animal {animal}')
    # if file format is .mat (MATLAB), then load with mat73.loadmat and convert necessary fields to numpy arrays
    if format == "MATLAB":
        dataset = loadmat(os.path.join(p_data, f"{animal}.mat"))
        # convert position, trace, and envs data from lists to numpy matrices
        for key in to_convert:
            dataset[key] = np.array(dataset[key])
        return {animal: dataset}
    # if file format is instead joblib, load the pre-saved joblib file version (previously with save_dat)
    elif format == "joblib":
        dataset = joblib.load(os.path.join(p_data, f"{animal}"))
        return dataset


def save_dat(dat, animal, p):
    """
    Save dataset for animal to desired path
    :param dat: dataset as generated with load_dat
    :param animal: animal ID (e.g., "QLAK-CA1-08")
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :return: None
    """
    p_data = os.path.join(p, "data")
    # save joblib file with compression
    joblib.dump(dat, os.path.join(p_data, animal), compress=3)


def mat2joblib(animal, p):
    """
    Load original MATLAB file and save joblib file with converted fields
    :param dat: dataset as generated with load_dat
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    """
    dat = load_dat(animal, p, format="MATLAB")
    save_dat(dat, animal, p)


def trace_sfps(SFPs):
    """
    Create contours from centred spatial footprints ("SFPs") previously registered across days with CellReg
    :param SFPs: x- and y-centred SFPs
    :return: SFP contours
    """
    SFPs = deepcopy(SFPs)
    n_cells, n_days = SFPs.shape[2], SFPs.shape[3]
    SFPs_traced = np.zeros_like(SFPs)
    for c in range(n_cells):
        for d in range(n_days):
            # create contours of SFPs by detecting changes in values at edge of solid object in SFPs matrices
            SFPs_bin = SFPs[:, :, c, d] > 1e-3
            # pop result for corresponding cell into numpy array
            SFPs_traced[:, :, c, d] = (np.vstack((np.zeros(SFPs_bin.shape[0])[np.newaxis], np.diff(SFPs_bin, axis=0))) +
                                       np.hstack((np.zeros(SFPs_bin.shape[1])[np.newaxis].T,
                                                  np.diff(SFPs_bin, axis=1)))).astype(bool)
    return SFPs_traced.astype(int)


def generate_behav_dict(animals, p, format="joblib"):
    """
    Generate and save dictionary containing behavioral data and environment information for all animals
    Dataset is smaller, easily loadable file
    :param animals: list of animal IDs to load
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param format: str file format to load original, whole dataset per animal from ("joblib" or "MATLAB")
    :return: None
    """
    p_data = os.path.join(p, "data")
    dict = {}
    for animal in animals:
        dat = load_dat(animal, p, format=format)
        dict[animal] = {'position': dat[animal]['position'],
                        'envs': dat[animal]['envs'],
                        'maps_shape': dat[animal]['maps']['smoothed'].shape}
        del dat
    joblib.dump(dict, os.path.join(p_data, "behav_dict"))


def get_environment_label(env_name, flipud=False):
    '''
    Create polygon marker with coordinates from environment vertices for plotting
    :param env_name: name of environment as string. Options are: "square", "o", "t", "u", "rectangle", "+", "i", "l",
    "bit donut", or "glenn"
    :arg flipud: option to vertically flip the polygon
    :return: matplotlib path for plotting, and polygon vertices (poly variable)
    '''
    codes = False
    # First draw vertices that will create the shape of the environment based on input name (env names in weirdGeos dat)
    if env_name == 'square':
        polys = np.array([[0, 0], [0, 30], [30, 30], [30, 0], [0, 0]])-15
    # o is a special case as it is only environment with a hole
    elif env_name == 'o':
        polys_outside = np.array([[0, 0], [0, 30], [30, 30], [30, 0], [0, 0]]) - 15
        polys_inside = np.array([[10, 10], [10, 21], [21, 21], [21, 10], [10, 10]]) - 15
        polys = np.concatenate((polys_outside[::1], polys_inside[::-1]))
        codes = np.ones(polys.shape[0], dtype=mpath.Path.code_type) * mpath.Path.LINETO
        codes[0] = mpath.Path.MOVETO
        codes[5] = mpath.Path.MOVETO
    elif env_name == 't':
        polys = np.array([[10, 0], [10, 21], [0, 21], [0, 30], [30, 30], [30, 21], [21, 21], [21, 0], [10, 0]]) - 15
    elif env_name == 'u':
        if flipud:
            polys = np.array([[0, 0], [0, 10], [21, 10], [21, 21], [0, 21], [0, 30], [30, 30], [30, 0], [0, 0]]) - 15
        else:
            polys = np.array([[0, 0], [30, 0], [30, 10], [10, 10], [10, 21], [30, 21], [30, 30], [0, 30], [0, 0]]) - 15
    elif env_name == 'rectangle':
        polys = np.array([[10, 0], [30, 0], [30, 30], [10, 30], [10, 0]]) - 15
    elif env_name == '+':
        polys = np.array([[0, 10], [10, 10], [10, 0], [21, 0], [21, 10], [30, 10], [30, 21], [21, 21], [21, 30],
                          [10, 30], [10, 21], [0, 21], [0, 10]]) - 15
    elif env_name == 'i':
        polys = np.array([[0, 0], [30, 0], [30, 10], [21, 10], [21, 21], [30, 21], [30, 30], [0, 30], [0, 21],
                          [10, 21], [10, 10], [0, 10], [0, 0]]) - 15
    elif env_name == 'l':
        if flipud:
            polys = np.array([[0, 0], [0, 10], [21, 10], [21, 30], [30, 30], [30, 0], [0, 0]]) - 15
        else:
            polys = np.array([[0, 0], [0, 30], [10, 30], [10, 10], [30, 10], [30, 0], [0, 0]]) - 15
    elif env_name == 'bit donut':
        if flipud:
            polys = np.array([[0, 0], [30, 0], [30, 21], [21, 21], [21, 10], [10, 10], [10, 21], [21, 21], [21, 30],
                              [0, 30], [0, 0]]) - 15
        else:
            polys = np.array([[0, 0], [30, 0], [30, 30], [10, 30], [10, 21], [21, 21], [21, 10], [10, 10], [10, 21],
                                  [0, 21], [0, 0]]) - 15
    elif env_name == 'glenn':
        if flipud:
            polys = np.array([[0,  0], [0, 21], [9, 21], [9, 30], [30, 30], [30, 9], [21, 9], [21, 0], [0, 0]]) - 15
        else:
            polys = np.array([[10, 0], [30, 0], [30, 21], [21, 21], [21, 30], [0, 30], [0, 10], [10, 10], [10, 0]]) - 15
    # Use Path function to create image path that matplotlib can plot
    if np.any(codes):
        if flipud:
            return mpath.Path(np.fliplr(polys), codes), -1 * polys
        else:
            return mpath.Path(np.fliplr(polys), codes), polys
    else:
        if flipud:
            return mpath.Path(np.fliplr(polys)), -1 * polys
        else:
            return mpath.Path(np.fliplr(polys)), polys


def get_env_mat(env):
    '''
    Get binary 3x3 matrix for the corresponding string environment name as floats, with 0 indicating omitted partitions.
    If invalid environment name is given will return 3x3 of nans.
    :param env: environment name as string. Options are: "square", "o", "t", "u", "rectangle", "+", "i", "l",
    "bit donut", or "glenn"
    :return: binary 3x3 matrix of the environment
    '''
    if env == 'square':
        return np.array([[1, 1, 1],
                         [1, 1, 1],
                         [1, 1, 1]]).astype(float)
    elif env == 'o':
        return np.array([[1, 1, 1],
                         [1, 0, 1],
                         [1, 1, 1]]).astype(float)
    elif env == 't':
        return np.array([[0, 1, 0],
                         [0, 1, 0],
                         [1, 1, 1]]).astype(float)
    elif env == 'u':
        return np.array([[1, 1, 1],
                         [1, 0, 0],
                         [1, 1, 1]]).astype(float)
    elif env == 'rectangle':
        return np.array([[0, 1, 1],
                         [0, 1, 1],
                         [0, 1, 1]]).astype(float)
    elif env == '+':
        return np.array([[0, 1, 0],
                         [1, 1, 1],
                         [0, 1, 0]]).astype(float)
    elif env == 'i':
        return np.array([[1, 1, 1],
                         [0, 1, 0],
                         [1, 1, 1]]).astype(float)
    elif env == 'l':
        return np.array([[1, 1, 1],
                         [1, 0, 0],
                         [1, 0, 0]]).astype(float)
    elif env == 'bit donut':
        return np.array([[1, 1, 1],
                         [1, 0, 1],
                         [0, 1, 1]]).astype(float)

    elif env == 'glenn':
        return np.array([[1, 1, 0],
                         [1, 1, 1],
                         [0, 1, 1]]).astype(float)
    else:
        print('Valid environment name not provided')
        return np.array([[np.nan, np.nan, np.nan],
                         [np.nan, np.nan, np.nan],
                         [np.nan, np.nan, np.nan]])


def get_transition_matrix(behav, maps, step_size=15, n_bins=15, buffer=1e-5):
    """
    Generate a transition matrix between states (spatial bins) in environment using animal behavior. Matrix will contain
    transition probabilities for each pair of states (bins) in the environment.
    :param behav: x-y position of animal in environment from dataset
    :param maps: rate maps used for rounding discrete behavioral states
    :param step_size: step in number of frames recorded at 30Hz recording to iterate across (e.g., every 15 steps = 2Hz)
    :param n_bins: number of spatial bins in x and y dimensions of the environment
    :param buffer: buffer size used for rounding discrete behavioral states
    :return: transition matrix as probabilities, and total counts of transitions between states
    """
    # make a deepcopy of behav and trace data as to not change original inputs
    behav = deepcopy(behav)
    # bin down behavioural data
    behav_max = behav.max(axis=0).max(axis=1)
    bin_down = (behav_max.max() + buffer) / n_bins
    behav /= bin_down
    # grab the number of days to iterate across, and initialize some variables to return and use as function params
    n_days = behav.shape[2]
    # create template maps for all days to round x-y position from behaviour
    temp_maps = np.nansum(maps, axis=2).astype(bool)
    # initialize transition matrix
    transition_mat = np.zeros([temp_maps.shape[0] * temp_maps.shape[1],
                               temp_maps.shape[0] * temp_maps.shape[1], n_days]).astype(float)
    transition_mat_counts = np.zeros_like(transition_mat)
    t1_map, t2_map = np.zeros([n_bins, n_bins]), np.zeros([n_bins, n_bins])
    for d in tqdm(range(n_days), desc=f'Calculating transition matrices across {n_days} days', leave=True, position=0):
        target_behav = np.floor(behav[::step_size, :, d]).astype(int)
        for i, (x, y) in enumerate(target_behav[:-1]):
            t1_map[x, y] += 1
            t2_map[target_behav[i+1].astype(int)[0], target_behav[i+1].astype(int)[1]] += 1
            transition_mat[np.argwhere(t1_map.ravel())[0][0], np.argwhere(t2_map.ravel())[0][0], d] += 1
            t1_map, t2_map = t1_map * 0, t2_map * 0
        # convert into probabilities that sum to one across column axis
        transition_mat_counts[:, :, d] = deepcopy(transition_mat[:, :, d])
        transition_mat[:, :, d] = (transition_mat[:, :, d].T / transition_mat[:, :, d].T.sum(axis=0)).T
    # return transition probabilities and count number
    return np.nan_to_num(transition_mat), transition_mat_counts


########################################################################################################################
# Rate map generation and split-half reliability measures
def get_rate_maps(position, trace, n_bins=15, fps=30, buffer=1e-5, filter_size=1.5):
    """
    Create rate maps from animal position and calcium trace data (single session).
    :param position: x-y position of the animal, shape n frames (row) by n dimensions (2 columns)
    :param trace: calcium trace data, shape n frames (row) by n cells
    :param n_bins: number of spatial bins in x- and y-dimensions
    :param fps: frames per second of recording
    :param buffer: buffer size for rounding binned position data
    :param filter_size: sigma size (bin number) for smoothing event rate maps
    :return: event rate maps, occupancy map (proportion of time in each bin), and average event rate for all cells
    """
    # time is row axis for position and trace data
    # First determine the length of the recording and number of neurons
    len_recording = trace.shape[0]
    n_cells = trace.shape[1]
    # Initialize rate maps and count map (which will count the number of times each bin is visited)
    rate_maps = np.zeros([n_cells, n_bins, n_bins])
    count_map = np.zeros([n_bins, n_bins])
    # Create a binned version of the position data with integer division
    position_binned = (position // ((np.nanmax(position, axis=0) + buffer) / n_bins)).astype(int)
    # Iterate through binned position data and add value of each trace
    # to rate map and a one to the same position on the count map
    for t, (x, y) in enumerate(position_binned):
        rate_maps[:, x, y] += trace[t, :]
        count_map[x, y] += 1
    nan_map = np.zeros_like(count_map) * np.nan
    nan_map[np.where(count_map)[0], np.where(count_map)[1]] = 1.
    if filter_size:
        rate_maps = gaussian_filter1d(gaussian_filter1d(rate_maps, sigma=filter_size, axis=1),
                                      sigma=filter_size, axis=2)
        count_map = gaussian_filter1d(gaussian_filter1d(count_map, sigma=filter_size, axis=0),
                                      sigma=filter_size, axis=1)
    # To determine the firing rate of neurons in each bin, divide by the number of
    # times the animal was in that location and multiply by the frames per second
    for cell in range(n_cells):
        rate_maps[cell] = (rate_maps[cell] / count_map) * fps * nan_map
    # Also create a probability map of dwell times in each bin to return
    occupancy_map = count_map / len_recording
    # get average event rate for all cells
    average_events = (np.sum(trace, axis=0) / trace.shape[0]) * fps
    return rate_maps, occupancy_map, average_events


def get_split_half(position, trace):
    """
    Calculate split-half reliability (Pearson correlation) from rate maps constructed with position and calcium traces.
    :param position: x-y position of the animal, shape n frames (row) by n dimensions (2 columns)
    :param trace: calcium trace data, shape n frames (row) by n cells
    :return: split-half correlation of event rate maps
    """
    # get the total length of the recording and number of cells
    len_recording = trace.shape[0]
    n_cells = trace.shape[1]
    # define first and second half session range in time series
    h1 = np.arange(len_recording / 2).astype(int)
    h2 = np.arange(len_recording / 2, len_recording).astype(int)
    # build rate maps for first and second session halves
    maps_h1, _, _ = get_rate_maps(position[h1], trace[h1])
    maps_h2, _, _ = get_rate_maps(position[h2], trace[h2])
    # get the number of bins in both x and y dimensions
    n_bins_x, n_bins_y = np.maximum(maps_h1.shape[1], maps_h2.shape[1]),\
                         np.maximum(maps_h1.shape[2], maps_h2.shape[2])
    # linearize rate maps for both session halves and assign them to first and second idx in last axis
    flat_maps = np.zeros([n_bins_x * n_bins_y, n_cells, 2])
    for cell in range(n_cells):
        flat_maps[:, cell, 0] = maps_h1[cell, :, :].flatten()
        flat_maps[:, cell, 1] = maps_h2[cell, :, :].flatten()
    # exclude spatial bins that were not visited in both session halves
    flat_maps[np.isnan(np.sum(flat_maps, axis=2)), :] = np.nan
    # correlate first and second session halves leaving out nan values
    map_corr = np.zeros(n_cells)
    for cell in range(n_cells):
        if np.any(~np.isnan(flat_maps[:, cell, :])):
            map_corr[cell], _ = pearsonr(x=flat_maps[~np.isnan(flat_maps[:, cell, 0]), cell, 0],
                                         y=flat_maps[~np.isnan(flat_maps[:, cell, 1]), cell, 1])
        else:
            map_corr[cell] = np.nan
    return map_corr


def get_shuffle_split_half(position, trace, nsims=1000, min_time=30, fps=30):
    """
    Calculate split-half reliability of event rate maps for circularly shuffled position and calcium traces.
    :param position: x-y position of the animal, shape n frames (row) by n dimensions (2 columns)
    :param trace: calcium trace data, shape n frames (row) by n cells
    :param nsims: number of shuffles to perform
    :param min_time: minimum time to circularly shift data in seconds
    :param fps: frames per second of original data
    :return: split-half correlation for shuffled data, shape n cells (rows) by nsims
    """
    # get the length of the recording and number of cells
    len_recording = trace.shape[0]
    n_cells = trace.shape[1]
    # initialize shuffle_corr
    shuffle_corr = np.zeros([n_cells, nsims])
    # circularly shuffle (roll) behavioural data randomly > min_time away from true time, and compute split-half corr
    for i in tqdm(range(nsims), leave=True, position=0, desc='Computing split-half reliability on shuffled data'):
        shuffled_position = np.roll(position, np.random.randint(min_time * fps, len_recording - min_time * fps), axis=0)
        shuffle_corr[:, i] = get_split_half(shuffled_position, trace)
    return shuffle_corr


def get_place_cells(position, trace, nsims=500, alpha=0.05):
    """
    Identify place cells based on split-half correlation of event rate maps.
    :param position: x-y position of the animal, shape n frames (row) by n dimensions (2 columns)
    :param trace: calcium trace data, shape n frames (row) by n cells
    :param nsims: number of shuffles to perform
    :param alpha: p-value threshold to consider cell as place cell
    :return: split-half reliability p-values for all cells, and place cells as boolean (n cells)
    """
    n_cells = trace.shape[1]
    # measure actual split-half
    sh_actual = get_split_half(position, trace)
    # measure shuffle split half
    sh_shuffle = get_shuffle_split_half(position, trace, nsims)
    # calculate p values
    p_vals = np.zeros(n_cells)
    place_cells = np.zeros(n_cells).astype(bool)
    for cell in range(n_cells):
        p_vals[cell] = 1 - ((sh_actual[cell] > sh_shuffle[cell, :]).sum() / nsims)
        if p_vals[cell] < alpha:
            if ~np.isnan(p_vals[cell]):
                place_cells[cell] = True
    p_vals[np.isnan(trace[0, :])] = np.nan
    place_cells[np.isnan(trace[0, :])] = np.nan
    return p_vals, place_cells


def get_shr_within(dat, animal, nsims=1000):
    """
    Iterate through days of recordings for animal, return within-session split-half reliability p-values and place cells
    :param dat: data dictionary for target animal as loaded with load_dat
    :param animal: animal ID as string (e.g., "QLAK-CA1-08")
    :param nsims: number of times to shuffle data to calculate p-values for reliability
    :return: split-half reliability p-values and place cells as boolean (n cells).
    """
    n_days = dat[animal]['trace'].shape[0]
    place_cells = [None] * n_days
    p_vals = [None] * n_days
    # iterate through days
    for day in range(n_days):
        # get split-half reliability and place cells for each day
        p_vals[day], place_cells[day] = get_place_cells(dat[animal]['position'][day].T,
                                                        dat[animal]['trace'][day].T, nsims=nsims)
    # convert to numpy array from list, and transpose
    p_vals, place_cells = np.array(p_vals).T, np.array(place_cells).T
    return p_vals, place_cells


def clean_rate_maps(maps, envs):
    """
    Clean up extraneous pixels for each environment (nans out)
    :param maps: pre-computed rate maps that have been cleaned
    :param envs: list of environment IDs as strings
    :return: cleaned rate maps
    """
    for d, env in enumerate(envs):
        map_mask = \
            np.fliplr(get_env_mat(env).T)[:, :, np.newaxis].repeat(25, axis=2).reshape(3, 3, 5, 5).transpose(0, 2, 1,
                                                                                                             3).reshape(
                15, 15). \
                astype(bool)
        for f in range(maps['smoothed'].shape[2]):
            maps['smoothed'][:, :, f, d][np.where(~map_mask)] = np.nan
            maps['unsmoothed'][:, :, f, d][np.where(~map_mask)] = np.nan
    return maps


def get_masked_maps(animal, dat):
    """
    Create rate maps for all recorded sessions using original maps but the shapes for each sequence will be generated
    from the first square of each geometric sequence in experiment.
    :param animal: animal ID, as a string (e.g. 'QLAK-CA1-08')
    :param dat: data dictionary for target animal as loaded with load_dat
    :return: masked maps as dictionary with the field 'smoothed' containing the output
    """
    n_days = dat[animal]['envs'].shape[0]
    # Identify square days in data (end of each geometric sequence)
    s_days = np.where(dat[animal]['envs'] == 'square')[0]
    # Index rate maps on square days
    s_maps = dat[animal]['maps']['smoothed'][:, :, :, s_days]
    # Initialize masked maps
    masked_maps = np.zeros_like(dat[animal]['maps']['smoothed']).transpose(3, 0, 1, 2) * np.nan
    s = 0
    cell_mask = np.zeros(s_maps.shape[2]).astype(bool)
    for d in range(n_days):
        mask = np.isnan(dat[animal]['maps']['smoothed'][:, :, :, d])
        # check if it is a square day
        if np.any(d == s_days):
            # iterate across all cells
            for c in range(s_maps.shape[2]):
                # if cell is registered on that day (non nans) and cell mask is not already filled
                if np.any(~mask[:, :, c]) and ~cell_mask[c]:
                    masked_maps[d:, :, :, c] = deepcopy(s_maps[:, :, c, s])[np.newaxis].repeat(n_days-d, axis=0)
                    cell_mask[c] = True
            s += 1
        masked_maps[d][mask] = np.nan
    masked_maps = {'smoothed': masked_maps.transpose(1, 2, 3, 0)}
    return masked_maps


def get_pv_vector_fields(animal, p, stable_simulation=False, place_cells=None):
    """
    Calculate the similarities of peak population vector correlation for each spatial bin in rate maps relative to first
    square day in each geometric sequence.
    :param animal: animal ID, as a string (e.g. 'QLAK-CA1-08')
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param stable_simulation: argument to simulate pv correlation with masked maps (simulates no remapping) as control
    :param place_cells: argument to use only split-half reliable cells (within session) in calculation
    :return: dictionary with average population vector correlation maps ("average") shape n bins x n bins x n dims (x-y)
    x number of environments x number of sequences, along with standard error maps, shape of environment, and population
    vector correlation matrix (n bins x n bins).
    """
    # load in dataset for animal
    dat = load_dat(animal, p, format="joblib")
    # get environment ID
    envs = dat[animal]['envs'].squeeze()
    # Option to perform control simulation based on first square day rate maps
    if stable_simulation:
        maps = get_masked_maps(animal, dat)["smoothed"]
    else:
        maps = dat[animal]['maps']['smoothed']
    n_bins, n_cells, n_days, n_envs = maps.shape[0], maps.shape[2], maps.shape[3], np.unique(envs).shape[0]
    # Fit days will be first and last square of each sequence
    fit_days = np.vstack((np.where(envs == 'square')[0][:-1], np.where(envs == 'square')[0][1:])).T
    # Initialize dictionary that will contain pv correlation maps, standard error, shapes, and pv correlation matrix
    pv_vector_fields_envs = {'average': np.zeros([n_bins, n_bins, 2, n_envs - 1, fit_days.shape[0]]),
                             'std': np.zeros([n_bins, n_bins, 2, n_envs - 1, fit_days.shape[0]]),
                             'shape': np.zeros([n_bins, n_bins, n_envs - 1]),
                             'pvmatrix': np.zeros([fit_days.shape[0], n_bins**2, n_bins**2, n_envs - 1])}
    # pv matrix will be the pv cross-correlation between two environments and all rate maps of cells registered on days
    for seq, (f1_idx, f2_idx) in enumerate(fit_days):
        for p_idx in np.arange(f1_idx, f2_idx)[1:]:
            # p_idx and p_maps refer to maps that we will correlate square days with
            f1_maps, f2_maps, p_maps = maps[:, :, :, f1_idx].reshape(n_bins**2, n_cells), \
                                      maps[:, :, :, f2_idx].reshape(n_bins**2, n_cells), \
                                      maps[:, :, :, p_idx].reshape(n_bins**2, n_cells)
            # if using only split-half reliable cells, nan out non-reliable cells from flattened maps
            if np.any(place_cells):
                f1_maps[:, ~place_cells[:, f1_idx]] = np.nan
                f2_maps[:, ~place_cells[:, f2_idx]] = np.nan
                p_maps[:, ~place_cells[:, p_idx]] = np.nan
            # Initialize pv correlations for all bins and cells
            pv_corrs = np.zeros([n_bins**2, n_bins**2, len([f1_maps, f2_maps])])
            for f, f_maps in enumerate([f1_maps, f2_maps]):
                nan_mask = np.where(np.logical_and(np.any(~np.isnan(f_maps), axis=0), np.any(~np.isnan(p_maps), axis=0)))[0]
                pv_corrs[:, :, f] = np.corrcoef(f_maps[:, nan_mask], p_maps[:, nan_mask])[:n_bins**2, n_bins**2:]
            pv_corrs = np.nanmean(pv_corrs, axis=2)
            pv_vector_fields_envs["pvmatrix"][seq, :, :, p_idx - f1_idx - 1] = pv_corrs
            # initialize two flattened maps with zeros to identify spatial bin of maximum correlation
            map1, map2 = np.zeros(n_bins**2), np.zeros(n_bins**2)
            pv_vector_fields = np.zeros([n_bins, n_bins, 2]) * np.nan
            for p, pv in enumerate(pv_corrs):
                # add one for the current index of session s1
                map1[p] += 1
                idx = np.argwhere(map1.reshape(n_bins, n_bins))[0]
                # add one for the max pv corr idx of s1 v s2
                if np.any(~np.isnan(pv)):
                    map2[np.argwhere(pv == np.nanmax(pv))[0][0]] += 1
                    pv_vector_fields[idx[0], idx[1]][:] = np.argwhere(map2.reshape(n_bins, n_bins))[0] - \
                                                          np.argwhere(map1.reshape(n_bins, n_bins))[0]
                # zero the maps
                map1 *= 0
                map2 *= 0
            for key in list(pv_vector_fields_envs.keys())[:-2]:
                pv_vector_fields_envs[key][:, :, :, np.where(envs[p_idx] == envs[1:np.unique(envs).shape[0]])[0][0],
                seq] = \
                    pv_vector_fields
    pv_vector_fields_envs['average'], pv_vector_fields_envs['std'] = np.nanmean(pv_vector_fields_envs['average'], -1), \
        np.nanstd(pv_vector_fields_envs['std'], -1)
    for e, env in enumerate(envs[1:n_envs]):
        pv_vector_fields_envs['shape'][:, :, e] = ~np.isnan(
            np.nanmean(np.nanmean(maps[:, :, :, np.where(envs == env)], 2),
                       -1).squeeze())
    pv_vector_fields_envs['envs'] = envs
    # clear data from the cache
    del dat
    return pv_vector_fields_envs


def get_pv_vector_fields_model(animal, p, feature_type):
    """
    Implementation of get_pv_vector_fields for model simulations. Same as there, but dictionary structure and data
    loading differ slightly.
    :param animal: animal ID, as a string (e.g. 'QLAK-CA1-08')
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param feature_type: name of model feature type as string (e.g. "PC" or "GC2PC") for data loading
    :return: dictionary with average population vector correlation maps ("average") shape n bins x n bins x n dims (x-y)
    x number of environments x number of sequences, along with standard error maps, shape of environment, and population
    vector correlation matrix (n bins x n bins).
    """
    p_models = os.path.join(p, "results", "riab")
    # Calculate population vector similarities within sequence relative to first square day
    envs = joblib.load(os.path.join(p, "data", 'behav_dict'))[animal]['envs']
    # maps = joblib.load(os.path.join(p_models, animal, "maps", f"{animal}_{feature_type}_maps"))["smoothed"]
    maps = joblib.load(os.path.join(p_models, f"{animal}_{feature_type}_maps"))["smoothed"]
    n_bins, n_cells, n_days, n_envs = maps.shape[0], maps.shape[2], maps.shape[3], np.unique(envs).shape[0]
    fit_days = np.vstack((np.where(envs == 'square')[0][:-1], np.where(envs == 'square')[0][1:])).T
    pv_vector_fields_envs = {'average': np.zeros([n_bins, n_bins, 2, n_envs - 1, fit_days.shape[0]]),
                             'std': np.zeros([n_bins, n_bins, 2, n_envs - 1, fit_days.shape[0]]),
                             'shape': np.zeros([n_bins, n_bins, n_envs - 1]),
                             'pvmatrix': np.zeros([fit_days.shape[0], n_bins**2, n_bins**2, n_envs - 1])}
    for seq, (f1_idx, f2_idx) in enumerate(fit_days):
        for p_idx in np.arange(f1_idx, f2_idx)[1:]:
            f1_maps, f2_maps, p_maps = maps[:, :, :, f1_idx].reshape(n_bins**2, n_cells), \
                                      maps[:, :, :, f2_idx].reshape(n_bins**2, n_cells), \
                                      maps[:, :, :, p_idx].reshape(n_bins**2, n_cells)
            pv_corrs = np.zeros([n_bins**2, n_bins**2, len([f1_maps, f2_maps])])
            for f, f_maps in enumerate([f1_maps, f2_maps]):
                nan_mask = \
                np.where(np.logical_and(np.any(~np.isnan(f_maps), axis=0), np.any(~np.isnan(p_maps), axis=0)))[0]
                pv_corrs[:, :, f] = np.corrcoef(f_maps[:, nan_mask], p_maps[:, nan_mask])[:n_bins**2, n_bins**2:]
            pv_corrs = np.nanmean(pv_corrs, axis=2)
            # also return the complete PV vector cross correlation
            pv_vector_fields_envs["pvmatrix"][seq, :, :, p_idx - f1_idx - 1] = pv_corrs

            # initialize two flattened maps with zeros
            map1, map2 = np.zeros(n_bins**2), np.zeros(n_bins**2)
            pv_vector_fields = np.zeros([n_bins, n_bins, 2]) * np.nan
            for p, pv in enumerate(pv_corrs):
                # add one for the current index of session s1
                map1[p] += 1
                idx = np.argwhere(map1.reshape(n_bins, n_bins))[0]
                # add one for the max pv corr idx of s1 v s2
                if np.any(~np.isnan(pv)):
                    map2[np.argwhere(pv == np.nanmax(pv))[0][0]] += 1
                    pv_vector_fields[idx[0], idx[1]][:] = np.argwhere(map2.reshape(n_bins, n_bins))[0] - \
                                                          np.argwhere(map1.reshape(n_bins, n_bins))[0]
                # zero the maps
                map1 *= 0
                map2 *= 0
            for key in list(pv_vector_fields_envs.keys())[:-2]:
                pv_vector_fields_envs[key][:, :, :, np.where(envs[p_idx] == envs[1:np.unique(envs).shape[0]])[0][0],
                seq] = \
                    pv_vector_fields
    pv_vector_fields_envs['average'], pv_vector_fields_envs['std'] = np.nanmean(pv_vector_fields_envs['average'], -1), \
        np.nanstd(pv_vector_fields_envs['std'], -1)
    for e, env in enumerate(envs[1:n_envs]):
        pv_vector_fields_envs['shape'][:, :, e] = ~np.isnan(
            np.nanmean(np.nanmean(maps[:, :, :, np.where(envs == env)], 2),
                       -1).squeeze()[:, :, 0])
    pv_vector_fields_envs['envs'] = envs
    return pv_vector_fields_envs


def get_vector_fields_animals(animals, p, stable_simulation=False, feature_type=False, place_cells_only=False,
                              alpha=0.01):
    """
    Iterate through all animals and calculate population vector correlations across all spatial bins in rate maps, build
    dictionary for all animals, along with calculating population vector correlations averaged across animals and
    standard deviation of correlation maps.
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param stable_simulation: argument to simulate pv correlation with masked maps (simulates no remapping) as control
    :param feature_type: name of model feature type as string (e.g. "PC" or "GC2PC") for data loading
    :param place_cells_only: argument to use only split-half reliable cells (within session) in calculation
    :param alpha: p-value threshold to consider cell as a place cell from split-half reliability measure
    :return: pv_vector_fields_animals - dictionary with pv correlation measures for all animals;  average_vector_fields
    mean - average vector fields across animals; std_vector_fields mean - standard deviation of vector fields across
    animals
    """
    # Iterate through all animals rate maps and determine population vector correlation fields to build dict
    pv_vector_fields_animals = {}
    for animal in animals:
        if feature_type:
            pv_vector_fields_animals[animal] = get_pv_vector_fields_model(animal, p, feature_type)
        else:
            if place_cells_only:
                shr = joblib.load(os.path.join(p, "results", f"{animal}_SHR"))
                pv_vector_fields_animals[animal] = get_pv_vector_fields(animal, p, place_cells=shr < alpha)
            else:
                pv_vector_fields_animals[animal] = get_pv_vector_fields(animal, p, stable_simulation)
    # Re-order field maps to match the ordering of first animal
    for a, animal in enumerate(animals):
        envs = pv_vector_fields_animals[animal]['envs']
        n_envs = np.unique(envs).shape[0]
        if a == 0:
            cannon_order = envs[1:n_envs]
        reorder_idx = np.zeros(n_envs-1).astype(int)
        for e, env in enumerate(cannon_order):
            reorder_idx[e] = np.where(envs[1:n_envs] == env)[0]
        for key in list(pv_vector_fields_animals[animal].keys())[:-1]:
            pv_vector_fields_animals[animal][key] = pv_vector_fields_animals[animal][key].T[reorder_idx].T
        pv_vector_fields_animals[animal]['envs'] = pv_vector_fields_animals[animal]['envs'][1:][reorder_idx]
    # Calculate averages and standard deviation across all animals, all sequences
    average_vector_fields = np.zeros(np.hstack((np.array(len(animals)),
                                                np.array(pv_vector_fields_animals[animal]['average'].shape))))
    std_vector_fields = np.zeros(np.hstack((np.array(len(animals)),
                                            np.array(pv_vector_fields_animals[animal]['std'].shape))))
    for a, animal in enumerate(animals):
        average_vector_fields[a] = pv_vector_fields_animals[animal]['average']
        std_vector_fields[a] = pv_vector_fields_animals[animal]['std']
    return pv_vector_fields_animals, average_vector_fields.mean(0), std_vector_fields.mean(0)


def get_pvcorr_pixelwise(animals, p, feature_type=None, shr_thresh=0.01,
                         average_result=True):
    """
    Calculate pixel-wise population vector correlation between all pairs of shapes for all spatially reliable cells
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param feature_type: name of model feature if not computed from ca1 data (None if computed from ca1 data)
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param shr_thresh: threshold for split-half reliability p-value for cells to be included
    :param average_result: whether to average result across all cells
    :return: population vector correlation matrix for all pairs of geometries across all animals across spatial bins
    """
    # First load behavioral data, which contains environment IDs, and set "cannon order" equal to first animal
    model_p = os.path.join("results", "riab")
    behav_dat = joblib.load(os.path.join(p, "data", "behav_dict"))
    cannon_order = behav_dat[animals[0]]["envs"]
    cannon_order = cannon_order[:np.unique(cannon_order).shape[0]].squeeze()
    for a, animal in tqdm(enumerate(animals)):
        # Now load in data from individual animal
        temp = load_dat(animal, p)
        # grab environment IDs for all recordings
        envs = temp[animal]['envs'].squeeze()
        # grab rate maps too
        if feature_type is None:
            maps = temp[animal]['maps']['smoothed']
            # also load split-half reliability to select place cells only
            shr = joblib.load(os.path.join(p, "results", f"{animal}_SHR"))
        else:
            maps = joblib.load(os.path.join(p, model_p, f"{animal}_{feature_type}_maps"))["smoothed"]
        del temp
        # measure number of bins, number of cells, number of days and unique environments
        n_bins, n_cells, n_days, n_envs = maps.shape[0], maps.shape[2], maps.shape[3], np.unique(envs).shape[0]
        # if using true dataset also nan out maps that are not split-half reliable
        if feature_type is None:
            for d in range(n_days):
                for c in range(n_cells):
                    if shr[c, d] >= shr_thresh:
                        maps[:, :, c, d] = np.nan
        # also get the days that index the start of each sequence (first square)
        s_idx = np.where(envs == "square")[0][:-1]
        # pv_matrix_envs will contain the cross-pv-correlation between all pairs of environments, in cannon order for
        # each sequence
        pv_matrix_envs = np.zeros([3, n_envs, n_envs, n_bins**2, n_bins**2]) * np.nan
        for n, s in enumerate(s_idx):
            # get maps and environment IDs just for target sequence
            seq_maps = maps.T[s:s + n_envs].T # transpose maps to simplify indexing
            seq_envs = envs[s:s + n_envs]
            for i, envA in enumerate(cannon_order):
                for j, envB in enumerate(cannon_order):
                    # get index within sequence for target environments to compare
                    a_idx, b_idx = np.where(seq_envs == envA)[0], np.where(seq_envs == envB)[0]
                    # grab maps for respective environments, and flatten them
                    a_maps, b_maps = (seq_maps[:, :, :, a_idx].reshape(n_bins**2, n_cells),
                                      seq_maps[:, :, :, b_idx].reshape(n_bins**2, n_cells))

                    # indices for cells that are registered in both environments
                    cell_mask = np.where(np.logical_and(np.any(~np.isnan(a_maps), axis=0),
                                                        np.any(~np.isnan(b_maps), axis=0)))[0]
                    pv_matrix_envs[n, i, j, :, :] = np.corrcoef(a_maps[:, cell_mask],
                                                                b_maps[:, cell_mask])[:n_bins ** 2, n_bins ** 2:]
            if np.logical_and(a == 0, n == 0):
                pv_matrix_animals = np.zeros([len(animals), s_idx.shape[0], n_envs, n_envs, n_bins**2, n_bins**2])
            pv_matrix_animals[a] = pv_matrix_envs
    if average_result:
        return np.nanmean(np.nanmean(pv_matrix_animals, 1), axis=0)
    else:
        return pv_matrix_animals

########################################################################################################################
# Representational similarity functions for RSM construction
def get_cell_rsm(maps, down_sample_mask=None, unsmoothed=False, d_thresh=0):
    """
    Compute similarity of event rate maps for cells registered across days.
    :param maps: event rate maps constructed from position data and calcium traces (precomputed in original dataset)
    :param down_sample_mask: option to downsample maps before computing similarity with masking
    :param unsmoothed: argument to use unsmoothed event rate maps
    :param d_thresh: threshold number of days to require cell to be registered in order to include in calculation
    :return: cell_rsm - cell rsm as numpy array of shape n days by n days by n cells; cell_idx - cell idx of which cells
    were included
    """
    # first load in either smoothed or unsmoothed rate maps depending on input arg
    if unsmoothed:
        # when loading in transpose the maps such that the cell and day axes come first
        maps = deepcopy(maps['unsmoothed']).transpose((2, 3, 0, 1))
    else:
        maps = deepcopy(maps['smoothed']).transpose((2, 3, 0, 1))
    # then use a down-sampling mask to nan out cells if down_sample_mask is
    # provided with a masking (n_cells, n_days)
    if np.any(down_sample_mask):
        maps[~down_sample_mask] = np.nan
    # transpose the rate maps back to their original order (xdim, ydim, ncell, ndays)
    maps = maps.transpose((2, 3, 0, 1))
    n_days = maps.shape[-1]
    # flatten the rate maps
    flat_maps = maps.reshape(maps.shape[0] * maps.shape[1], maps.shape[2], maps.shape[3])
    # create a mask to avoid trying to correlate nans
    nan_idx = np.isnan(flat_maps)
    reg_idx = np.any(~nan_idx, axis=0)
    cell_idx = np.where(reg_idx.sum(axis=1) >= d_thresh)[0]
    cell_rsm = np.zeros([n_days, n_days, cell_idx.shape[0]])
    # iterate across all pairs of days, correlating event rate maps for each cell across registered days
    for c, cell in tqdm(enumerate(cell_idx), leave=True, position=0,
                        desc='Correlating ratemaps across days and cell pairs'):
        for s1 in range(n_days):
            for s2 in range(n_days):
                s1_map, s2_map = flat_maps[:, cell, s1], flat_maps[:, cell, s2]
                if np.any(~np.isnan(s1_map)) and np.any(~np.isnan(s2_map)):
                    nan_mask = ~np.isnan(np.vstack((s1_map, s2_map)).sum(axis=0))
                    cell_rsm[s1, s2, c] = pearsonr(s1_map[nan_mask],
                                                   s2_map[nan_mask])[0]
                else:
                    cell_rsm[s1, s2, c] = np.nan
    return cell_rsm, cell_idx


def get_mean_map_corr(animals, p):
    """
    Compute average event rate map correlation for all animals, matching order of environments, and only correlating
    cells within animal that are registered across days. Simply a wrapper for get_cell_rsm that reorders rate maps
    to match the sequence of shapes across days.
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :return: mean_map_corr - average event rate correlation across animals; cannon_order - cannon order of environments
    (defined by first animal's sequence); labels - environment labels; map_corr_animals_sequences - map correlations
    without averaging across animals and sequences.
    """
    # build rate map correlations across geometries, averaging first within animals across sequences, then across animal
    map_corr_animals_sequences = np.zeros([len(animals), 3, 11, 11]) * np.nan
    for a, animal in enumerate(animals):
        temp = load_dat(animal, p, format="joblib")
        maps, envs = temp[animal]["maps"], temp[animal]["envs"].ravel()
        s_days = np.vstack((np.where(envs == "square")[0][:-1], np.where(envs == "square")[0][1:])).T
        if a == 0:
            cannon_order = envs[1:10]
            labels = np.array(["square"] + list(cannon_order) + ["square"])
        else:
            for s1, s2 in s_days:
                # get index for reordering of environments to match fist animal (cannon order)
                reorder_idx = np.array([np.where(envs[s1+1:s2] == e)[0]+(s1+1) for e in cannon_order]).ravel()
                maps["smoothed"] = maps["smoothed"].transpose()
                maps["smoothed"][s1+1:s2] = maps["smoothed"][reorder_idx]
                maps["smoothed"] = maps["smoothed"].T
        # build rsm
        map_corr, _ = get_cell_rsm(maps)
        for seq, (s1, s2) in enumerate(s_days):
            map_corr_animals_sequences[a, seq, :, :] = np.nanmean(map_corr[s1:s2 + 1, s1:s2 + 1], axis=-1)
            if seq == 0:
                seq_mean = np.nanmean(map_corr[s1:s2+1, s1:s2+1], axis=-1)[:, :, np.newaxis]
            else:
                seq_mean = np.dstack((seq_mean, np.nanmean(map_corr[s1:s2+1, s1:s2+1], axis=-1)[:, :, np.newaxis]))

        if a == 0:
            animals_mean = np.nanmean(seq_mean, axis=-1)[:, :, np.newaxis]
        else:
            animals_mean = np.dstack((animals_mean, np.nanmean(seq_mean, axis=-1)[:, :, np.newaxis]))
    mean_map_corr = animals_mean.mean(-1)
    return mean_map_corr, cannon_order, labels, map_corr_animals_sequences


def get_rsm_similarity_animals_sequences(animals, p, nsims=100, s_prop=0.9):
    """
    Compute the true and shuffled similarity of map correlation across animals for all pairs of enviornments for each
    geometric sequence.
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param nsims: number of times to perform bootstrapping similarity calculatino and shuffling of data
    :param s_prop: proportion of data to use in similarity calculation bootstrapping (resampling)
    :return: df_map_corr_animals_sequences - dataframe of bootstrapped correlation values of similarity across animals
    within each sequence
    """
    # set random seed for shuffling
    np.random.seed(2023)
    #
    map_corr_envs = joblib.load(os.path.join(p, "results", "map_corr_envs"))
    map_corr_animals_sequences = map_corr_envs["map_corr_animals_sequences"]
    cols = ["Animal A", "Animal B", "Sequence", "Fit", "Shuffle"]
    map_corr_animal_sequence_similarity = np.zeros([len(animals) ** 2 * 3 * nsims * 2, len(cols)]) * np.nan
    c = 0
    for s in range(map_corr_animals_sequences.shape[1]):
        for a1 in tqdm(range(len(animals)), desc="Calculating rate map correlation rsm similarity across animals",
                       position=0, leave=True):
            for a2 in range(len(animals)):
                temp_corr1, temp_corr2 = map_corr_animals_sequences[a1, s, :, :], map_corr_animals_sequences[a2, s, :,
                                                                                  :]
                for i in range(nsims):
                    choice_idx = np.argwhere(np.tri(temp_corr1.shape[0], k=-1))
                    choice_idx = choice_idx[np.random.choice(np.arange(choice_idx.shape[0]),
                                                             size=int(choice_idx.shape[0] * s_prop),
                                                             replace=True)]
                    if np.any(~np.isnan(temp_corr1)) and np.any(~np.isnan(temp_corr2)):
                        actual = kendalltau(temp_corr1[choice_idx], temp_corr2[choice_idx])[0]
                        shuffle = kendalltau(np.random.permutation(temp_corr1)[choice_idx], temp_corr2[choice_idx])[0]
                        map_corr_animal_sequence_similarity[c] = np.hstack((a1, a2, s + 1, actual, False))
                        c += 1
                        map_corr_animal_sequence_similarity[c] = np.hstack((a1, a2, s + 1, shuffle, True))
                        c += 1

    df_map_corr_animals_sequences = pd.DataFrame(data=map_corr_animal_sequence_similarity, columns=cols)
    df_map_corr_animals_sequences.dropna(axis=0, inplace=True)
    df_map_corr_animals_sequences = df_map_corr_animals_sequences[df_map_corr_animals_sequences["Animal A"] !=
                                                                  df_map_corr_animals_sequences["Animal B"]]
    return df_map_corr_animals_sequences


def get_cell_rsm_partitioned(maps, down_sample_mask=None, d_thresh=0):
    """
    Compute similarity of 3x3 partitioned event rate maps for all cells (pairwise) registered across sessions.
    :param maps: event rate maps shape n bins x n bins (expects 15 x 15) x n cells x n days
    :param down_sample_mask: optional down sampling mask to match minimum number of cells across all days
    :param d_thresh: optional param to threshold minimum number of days cell must be registered to be included
    :return: cell_rsm - similarity matrix for all pairwise similarities within cells for all partitions; labels - labels
    for day and parition ID corresponding to each pairwise comparison in cell_rsm; cell_idx - which cells were included
    in analysis
    """
    # first load in either smoothed or unsmoothed rate maps depending on input arg
    # when loading in transpose the maps such that the cell and day axes come first
    maps = deepcopy(maps['smoothed']).transpose((2, 3, 0, 1))
    # then use a down-sampling mask to nan out cells if down_sample_mask is
    # provided with a masking (ncells, ndays)
    if np.any(down_sample_mask):
        maps[~down_sample_mask] = np.nan
    # transpose the rate maps back to their original order (xdim, ydim, ncell, ndays)
    maps = maps.transpose((2, 3, 0, 1))
    n_days = maps.shape[-1]
    # build partition masks for 3x3 grid comparisons (5 x 5 partitions of the 15 x 15 space)
    parts_idx = ((0, 5), (5, 10), (10, 15))
    n_parts = len(parts_idx) ** 2
    parts_mask = np.zeros((len(parts_idx)**2, maps.shape[0], maps.shape[1])).astype(bool)
    count = 0
    for x1, x2 in parts_idx:
        for y1, y2 in parts_idx:
            parts_mask[count, x1:x2, y1:y2] = True
            count += 1
    # flatten the rate maps create a mask to avoid correlating nans
    flat_maps = maps.reshape(maps.shape[0] * maps.shape[1], maps.shape[2], maps.shape[3])
    nan_idx = np.isnan(flat_maps)
    reg_idx = np.any(~nan_idx, axis=0)
    # cell_idx will show which cells were included that pass the number of days threshold (d_thresh)
    cell_idx = np.where(reg_idx.sum(axis=1) >= d_thresh)[0]
    # initialize cell-wise, partition-wise rsm
    cell_rsm = np.zeros([n_days, n_parts, n_days, n_parts, cell_idx.shape[0]])
    for c, cell in tqdm(enumerate(cell_idx), leave=True, position=0,
                        desc='Correlating partitions of rate maps across days and cell pairs'):
        for s1 in range(n_days):
            for s2 in range(n_days):
                # break similarities down by parition-wise comparison
                for p1_idx, p1_mask in enumerate(parts_mask):
                    for p2_idx, p2_mask in enumerate(parts_mask):
                        p1_map, p2_map = maps[:, :, cell, s1][p1_mask], maps[:, :, cell, s2][p2_mask]
                        if np.logical_and(np.where(~np.isnan(p1_map))[0].shape[0] >= 5,
                                          np.where(~np.isnan(p2_map))[0].shape[0] >= 5):
                            nan_mask = ~np.isnan(np.vstack((p1_map, p2_map)).sum(axis=0))
                            if nan_mask.astype(int).sum() > 1:
                                cell_rsm[s1, p1_idx, s2, p2_idx, c] = pearsonr(p1_map[nan_mask],
                                                                               p2_map[nan_mask])[0]
                        else:
                            cell_rsm[s1, p1_idx, s2, p2_idx, c] = np.nan
    # reshape cell_rsm to be n days x n parts x n_cells
    cell_rsm = cell_rsm.reshape(n_parts * n_days, n_parts * n_days, -1)
    # labels will have number labels for day and partition along each axis
    labels = np.zeros([n_days * n_parts, 2])
    c = 0
    for s in range(n_days):
        for p in range(n_parts):
            labels[c, :] = np.array([s, p])
            c += 1
    return cell_rsm, labels, cell_idx


def get_rsm_partitioned_sequences(animals, p, file_ext='rsm_partitioned'):
    '''
    Generate partitioned similairity measures (averaged over cells) by sequence for animals and store in nested
    dictionary along with labels and sorted in cannon order (defined by first animal's geometric sequence).
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param file_ext: file name extension for saved results from cell rsm dictionary
    :return: rsm_animals - dictionary of partitioned similarity for all animals
    '''
    rsm_animals = {}
    for animal in animals:
        print(animal)
        # load the cell- and partion-wise RSM previously calculated for each animal
        rsm_dict = joblib.load(os.path.join(p, "results", f'{animal}_{file_ext}'))
        rsm_animals[animal] = {'cell_idx': rsm_dict['cell_idx'], 'rsm': None}
        # if agg (aggregate) then average across cell axis (last) if not already using agg (PV corr) rsm
        rsm_average = rsm_dict['RSM']
        # get number of days, partitions, shapes, and sequences
        n_days = rsm_dict['envs'].shape[0]
        n_parts = rsm_average.shape[0] // n_days
        n_shapes = np.unique(rsm_dict['envs']).shape[0]
        n_seq = n_days // n_shapes
        n_cells = rsm_average.shape[-1]
        # if first animal get 'cannon' labels for sorting other animals partitioned sequences
        if animal == animals[0]:
            rsm_animals['cannon_labels'] = np.vstack((np.tile(rsm_dict['envs'][np.newaxis].T, n_parts).ravel(),
                                                      rsm_dict['p_labels'])).T
        # target labels are those for actual animal under consideration
        target_labels = np.vstack((np.tile(rsm_dict['envs'][np.newaxis].T, n_parts).ravel(),
                                   rsm_dict['p_labels'])).T
        # get start and stop idx for deformed shapes (since squares will stay in place)
        def_idx = (n_parts, n_shapes * n_parts)
        # create sorting index for target rsm relative to cannon that can be applied to each sequence separately
        sort_idx = np.array([np.where(np.all(rsm_animals['cannon_labels'][e] == target_labels[def_idx[0]:def_idx[1]],
                                             axis=1))[0]
                             for e in range(def_idx[0], def_idx[1])]).ravel() + n_parts
        # concatenate on square indices now that deformed shapes are given indices
        sort_idx = np.hstack((np.arange(0, n_parts), sort_idx, np.arange(n_shapes * n_parts, (n_shapes + 1) * n_parts)))
        # sorted rsm will be number of partitions in each sequence (square to square) ** 2 by the number of sequences
        rsm_animals[animal]['rsm'] = np.zeros([n_seq, (n_shapes + 1) * n_parts, (n_shapes + 1) * n_parts, n_cells])\
                                     * np.nan
        square_idx = np.where(rsm_dict['envs'] == 'square')[0] * n_parts
        for i in range(square_idx[:-1].shape[0]):
            j, k = square_idx[i:i+2]
            k += n_parts
            rsm_animals[animal]['rsm'][i, :, :] = rsm_average[j:k, j:k][sort_idx, :][:, sort_idx]
    rsm_animals['cannon_labels'] = rsm_animals['cannon_labels'][:n_parts * 11]
    return rsm_animals


def calculate_sequence_similarity(rsm_parts_ordered):
    """
    Calculate second-order similarity of paritioned event rate maps similarities across geometric sequences
    :param rsm_parts_ordered: cell rsm ordered to match partitions and geometries across animals, calculated with
    get_rsm_partitioned_similarity
    :return: sequence_similarity with similarities across sequences of geometries in dataframe
    """
    # First calculate similarity of each animal to itself across sequences
    n_seq, n_animals, n_shuffles, n_deviations = rsm_parts_ordered.shape[0], rsm_parts_ordered.shape[1], 10000, 100
    sequence_similarity = {'Tau': np.zeros([n_seq, n_seq, n_animals]) * np.nan,
                           'p': np.zeros([n_seq, n_seq, n_animals]) * np.nan,
                           'SE': np.zeros([n_seq, n_seq, n_animals])}
    for s1 in tqdm(range(n_seq)):
        for s2 in range(n_seq):
            for a in range(n_animals):
                if np.any(~np.isnan(rsm_parts_ordered[s1, a, :, :])) and np.any(~np.isnan(rsm_parts_ordered[s2, a, :, :])):
                    # grab target rsms for comparison
                    rsm1, rsm2 = rsm_parts_ordered[s1, a, :, :], rsm_parts_ordered[s2, a, :, :]
                    # take lower triangle of each
                    rsm1, rsm2 = rsm1[np.tri(rsm1.shape[0], k=-1).astype(bool)], rsm2[np.tri(rsm2.shape[0], k=-1).astype(bool)]
                    # generate mask to remove nans
                    nan_mask = ~np.isnan(rsm1 + rsm2)
                    # remove nans
                    rsm1, rsm2 = rsm1[nan_mask], rsm2[nan_mask]
                    # calculate actual pearson correlation
                    true_corr = kendalltau(rsm1, rsm2)[0]
                    sequence_similarity['Tau'][s1, s2, a] = true_corr
                    # calculate standard error for each comparison using bootstrap procedure
                    for i in range(n_deviations):
                        # create idx for random draws with replacement from rsms
                        choice_idx = np.sort(np.random.choice(np.arange(rsm1.shape[0]), size=int(rsm1.shape[0]),
                                                              replace=True))
                        # calculate deviation from true correlation
                        sequence_similarity['SE'][s1, s2, a] += (true_corr -
                                                                 kendalltau(rsm1[choice_idx], rsm2[choice_idx])[0]) ** 2
                    # compute SE from deviations
                    sequence_similarity['SE'][s1, s2, a] = np.sqrt(sequence_similarity['SE'][s1, s2, a] /
                                                                   (n_deviations - 1))
                    # calculate shuffled correlations to estimate p value
                    shuffle_corr = np.zeros(n_shuffles) * np.nan
                    for i in range(n_shuffles):
                        shuffle_idx = np.random.permutation(rsm2.shape[0])
                        shuffle_corr[i] = kendalltau(rsm1, rsm2[shuffle_idx])[0]
                    sequence_similarity['p'][s1, s2, a] = (n_shuffles - (true_corr > shuffle_corr).sum())/n_shuffles
    return sequence_similarity


def calculate_animal_similarity(rsm_parts_ordered):
    """
    Calculate second-order similarity of paritioned event rate maps similarities across animals within each sequences
    :param rsm_parts_ordered: cell rsm ordered to match partitions and geometries across animals, calculated with
    get_rsm_partitioned_similarity
    :return: animal_similarity with similarities across animals within each sequence of geometries in a dataframe
    """
    n_seq, n_animals, n_shuffles, n_deviations = (rsm_parts_ordered.shape[0], rsm_parts_ordered.shape[1], 10000, 100)
    animal_similarity = {'Tau': np.zeros([n_animals, n_animals, n_seq]) * np.nan,
                         'p': np.zeros([n_animals, n_animals, n_seq]) * np.nan,
                         'SE': np.zeros([n_animals, n_animals, n_seq])}
    for s in range(n_seq):
        for a1 in tqdm(range(n_animals)):
            for a2 in range(n_animals):
                if np.any(~np.isnan(rsm_parts_ordered[s, a1, :, :])) and np.any(~np.isnan(rsm_parts_ordered[s, a2, :, :])):
                    # grab target rsms for comparison
                    rsm1, rsm2 = rsm_parts_ordered[s, a1, :, :], rsm_parts_ordered[s, a2, :, :]
                    # take lower triangle of each
                    rsm1, rsm2 = rsm1[np.tri(rsm1.shape[0], k=-1).astype(bool)], rsm2[np.tri(rsm2.shape[0], k=-1).astype(bool)]
                    # generate mask to remove nans
                    nan_mask = ~np.isnan(rsm1 + rsm2)
                    # remove nans
                    rsm1, rsm2 = rsm1[nan_mask], rsm2[nan_mask]
                    # calculate actual pearson correlation
                    true_corr = kendalltau(rsm1, rsm2)[0]
                    animal_similarity['Tau'][a1, a2, s] = true_corr
                    # calculate standard error for each comparison using bootstrap procedure
                    for i in range(n_deviations):
                        # create idx for random draws with replacement from rsms
                        choice_idx = np.sort(np.random.choice(np.arange(rsm1.shape[0]), size=int(rsm1.shape[0]),
                                                              replace=True))
                        # calculate deviation from true correlation
                        animal_similarity['SE'][a1, a2, s] += (true_corr -
                                                                   kendalltau(rsm1[choice_idx], rsm2[choice_idx])[0]) ** 2
                    # compute SE from deviations
                    animal_similarity['SE'][a1, a2, s] = np.sqrt(animal_similarity['SE'][a1, a2, s] /
                                                                 (n_deviations - 1))
                    # calculate shuffled correlations to estimate p value
                    shuffle_corr = np.zeros(n_shuffles) * np.nan
                    for i in range(n_shuffles):
                        shuffle_idx = np.random.permutation(rsm2.shape[0])
                        shuffle_corr[i] = kendalltau(rsm1, rsm2[shuffle_idx])[0]
                    animal_similarity['p'][a1, a2, s] = (n_shuffles - (true_corr > shuffle_corr).sum())/n_shuffles
        np.fill_diagonal(animal_similarity['Tau'][:, :, s], np.nan)
    return animal_similarity


def get_rsm_partitioned_similarity(rsm_parts_animals, animals, get_sequence_similarity=True, get_animal_similarity=True,
                                   subsample=False, n_cells=100, n_reps=100):
    '''
    Calculate second-order similarities in event rate maps across geometric sequences, across animals within sequence,
    and order paritioned similarity matrices and average resulting matrices across animals.
    :param rsm_parts_animals: partitioned similarities across animals, computed with get_rsm_partitioned_sequences
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :arg get_sequence_similarity: whether to calculate similarities across sequences
    :arg get_animal_similarity: whether to calculate similarities across animals
    :arg subsample: whether to include a defined number of cells across animals with bootstrap resampling procedure
    :arg n_cells: number of cells to include in subsample if subsample is true
    :param n_reps: number of times to resample if subsample is true
    :return: sequence_similarity - dataframe of similarities across seqeunce (optional); animal_similarity - dataframe
    of similarities across animals (optional); rsm_parts_ordered, rsm_parts_averaged
    '''
    if not subsample:
        # collect all rsms into each sequence, ordered by animal
        rsm_parts_ordered = np.nan * np.zeros([rsm_parts_animals[animals[0]]['rsm'].shape[0], len(animals),
                                               rsm_parts_animals[animals[0]]['rsm'].shape[1],
                                               rsm_parts_animals[animals[0]]['rsm'].shape[2]])
        for a, animal in enumerate(animals):
            n_seq = rsm_parts_animals[animal]['rsm'].shape[0]
            for s in range(n_seq):
                rsm_parts_ordered[s, a] = np.nanmean(rsm_parts_animals[animal]['rsm'][s], -1)
        rsm_parts_averaged = np.nanmean(np.nanmean(rsm_parts_ordered, 0), 0)
        if get_sequence_similarity:
            sequence_similarity = calculate_sequence_similarity(rsm_parts_ordered)
        if get_animal_similarity:
            animal_similarity = calculate_animal_similarity(rsm_parts_ordered)
        if get_sequence_similarity and get_animal_similarity:
            return sequence_similarity, animal_similarity, rsm_parts_ordered, rsm_parts_averaged
        else:
            return rsm_parts_ordered, rsm_parts_averaged
    else:
        # if subsample is true, generate similarities from sub-sampling cells
        rsm_parts_ordered = np.nan * np.zeros([n_reps, rsm_parts_animals[animals[0]]['rsm'].shape[0], len(animals),
                                               rsm_parts_animals[animals[0]]['rsm'].shape[1],
                                               rsm_parts_animals[animals[0]]['rsm'].shape[2]])
        for r in range(n_reps):
            for a, animal in enumerate(animals):
                sub_idx = np.random.choice(np.arange(rsm_parts_animals[animal]['rsm'][0].shape[-1]),
                                           size=n_cells,
                                           replace=True)
                n_seq = rsm_parts_animals[animal]['rsm'].shape[0]
                for s in range(n_seq):

                    rsm_parts_ordered[r, s, a] = np.nanmean(rsm_parts_animals[animal]['rsm'][s][:, :, sub_idx], -1)
        rsm_parts_averaged = np.nanmean(np.nanmean(rsm_parts_ordered, 1), 1)
        return rsm_parts_ordered, rsm_parts_averaged


def predict_rsm_animals(animals, rsm_parts_animals):
    """
    Try to predict similarity values for all geometry-partition pairs in environments across animals with linear
    classifier, and predict animal ID with categorical naive bayes.
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param rsm_parts_animals: partitioned similarities across animals, computed with get_rsm_partitioned_sequences
    :return: df_animal_similarity - dataframe for predictions of rsm across animals for each sequence with linear model;
    df_animal_ID - dataframe for prediction of animal ID with categorical naive bayes; df_animals - datafrane of
    similarities across animnals; df_sequences - dataframe for similarities across sequences
    """
    rsm_parts_ordered, rsm_parts_averaged = get_rsm_partitioned_similarity(rsm_parts_animals, animals,
                                                                           False, False)
    if rsm_parts_animals[animals[0]]['rsm'].shape[-1] > rsm_parts_animals[animals[0]]['rsm'].shape[-2]:
        for animal in animals:
            rsm_parts_animals[animal]['rsm'] = np.nanmean(rsm_parts_animals[animal]['rsm'], axis=-1)
    cols = ["Animal 1", "Animal 2", "Sequence", "Fit", "Data"]
    df_animal_similarity = pd.DataFrame(data=np.zeros([2* np.prod(rsm_parts_ordered.shape[0] * rsm_parts_ordered.shape[1] **2),
                                                       len(cols)]) * np.nan, columns=cols)
    c = 0
    for s in range(rsm_parts_ordered.shape[0]):
        for a1 in range(rsm_parts_ordered.shape[1]):
            for a2 in range(rsm_parts_ordered.shape[1]):
                if a1 != a2:
                    rsm1, rsm2, rsm_shuff1, rsm_shuff2 = \
                        deepcopy(rsm_parts_ordered[s, a1]), deepcopy(rsm_parts_ordered[s, a2]), \
                            deepcopy(rsm_parts_ordered[s, a1]), \
                            deepcopy(rsm_parts_ordered[s, a2])
                    if np.any(~np.isnan(rsm1)) and np.any(~np.isnan(rsm2)):
                        nan_mask = ~np.isnan(rsm1[np.eye(rsm1.shape[0]).astype(bool)] +
                                             rsm2[np.eye(rsm2.shape[0]).astype(bool)])
                        rsm1, rsm2, rsm_shuff1, rsm_shuff2 = rsm1[nan_mask, :][:, nan_mask], rsm2[nan_mask, :][:, nan_mask],\
                                     rsm_shuff1[nan_mask, :][:, nan_mask], rsm_shuff2[nan_mask, :][:, nan_mask]
                        rsm1, rsm2, rsm_shuff1, rsm_shuff2 =\
                            rsm1[np.tri(rsm1.shape[0], k=-1).astype(bool)],\
                            rsm2[np.tri(rsm2.shape[0], k=-1).astype(bool)],\
                            rsm_shuff1[np.tri(rsm_shuff1.shape[0], k=-1).astype(bool)],\
                            rsm_shuff2[np.tri(rsm_shuff2.shape[0], k=-1).astype(bool)]

                        df_animal_similarity.iloc[c] = np.hstack((a1, a2, s, kendalltau(rsm1, rsm2)[0], 0))
                        c += 1
                        df_animal_similarity.iloc[c] = np.hstack((a1, a2, s,
                                                                  kendalltau(np.random.permutation(rsm_shuff1),
                                                                             np.random.permutation(rsm_shuff2))[0],
                                                                  1))
                        c += 1
    df_animal_similarity['Data'][df_animal_similarity['Data'] == 0] = "Actual"
    df_animal_similarity['Data'][df_animal_similarity['Data'] == 1] = "Shuffle"
    df_animal_similarity.dropna(axis=0, inplace=True)
    cols = ["Animal", "Sequence", "Correct", "Data"]
    df_animal_ID = pd.DataFrame(data=np.zeros([2*(np.prod(rsm_parts_ordered.shape[:2]) -1), len(cols)]), columns = cols)
    # fit linear model from rsms to predict animal ID in each sequence
    # for each sequence, fit linear model on remaining sequences
    # then predict animal on left out sequence
    c = 0
    for a1, animal1 in enumerate(animals):
        # calculate number of sequences for each animal that we are trying to predict
        n_seq = rsm_parts_animals[animal1]['rsm'].shape[0]
        for s1 in range(n_seq):
            X_fit = []
            y_fit = []
            for a2, animal2 in enumerate(animals):
                for s2 in range(rsm_parts_animals[animal2]['rsm'].shape[0]):
                    # build model data by flattening rsm for all animals from non-s1 sequences, all animals
                    # there will be a single label for each flattened rsm (animal id)
                    if s1 != s2:
                        rsm_copy = deepcopy(rsm_parts_animals[animal2]['rsm'][s2])
                        nan_mask = ~np.isnan(rsm_copy[np.eye(rsm_copy.shape[0]).astype(bool)])
                        rsm_copy = rsm_copy[nan_mask, :][:, nan_mask]
                        y_fit.append(a2)
                        X_fit.append(rsm_copy.ravel())
            X_fit, y_fit = np.array(X_fit), np.array(y_fit)
            cgb_actual = CategoricalNB(force_alpha=True)
            cgb_actual.fit(X=X_fit, y=y_fit)

            rsm_copy = deepcopy(rsm_parts_animals[animal1]['rsm'][s1])
            nan_mask = ~np.isnan(rsm_copy[np.eye(rsm_copy.shape[0]).astype(bool)])
            rsm_copy = rsm_copy[nan_mask, :][:, nan_mask]
            df_animal_ID.iloc[c] = np.hstack((a1, s1, cgb_actual.predict(rsm_copy.ravel()[np.newaxis])==a1, 0))
            c += 1
            df_animal_ID.iloc[c] = np.hstack((a1, s1, cgb_actual.predict(np.random.permutation(rsm_copy.ravel()[
                                                                                                   np.newaxis])) == a1,
                                   1))
            c += 1
    df_animal_ID['Data'][df_animal_ID['Data'] == 0] = "Actual"
    df_animal_ID['Data'][df_animal_ID['Data'] == 1] = "Shuffle"

    cols = ["Animal", "Sequence", "Decoding Accuracy", "Data"]
    df_animals = pd.DataFrame(data=np.zeros([2*(np.prod(rsm_parts_ordered.shape[:2]) -1), len(cols)]), columns = cols)
    # fit label encoder to 'cannon labels'
    onehot = OneHotEncoder(sparse_output=False)
    onehot.fit(rsm_parts_animals['cannon_labels'])
    # fit a linear model to rsm from all animals, and predict rsm left out
    c=0
    for a1, animal1 in enumerate(animals):
        n_seq = rsm_parts_animals[animal1]['rsm'].shape[0]
        for s in range(n_seq):
            X = []
            y = []
            for a2, animal2 in enumerate(animals):
                # if not target animal, add rsm to fit data
                if a1 != a2:
                    if n_seq <= rsm_parts_animals[animal2]['rsm'].shape[0]:
                        rsm_copy = deepcopy(rsm_parts_animals[animal2]['rsm'][s])
                        nan_mask = ~np.isnan(rsm_copy[np.eye(rsm_copy.shape[0]).astype(bool)])
                        rsm_copy = rsm_copy[nan_mask, :][:, nan_mask]
                        y.append(rsm_copy)
                        X.append(onehot.transform(rsm_parts_animals['cannon_labels'][nan_mask]))
            X, y = np.array(X), np.array(y)
            X, y = X.reshape(-1, X.shape[-1]), y.reshape(-1, y.shape[-1])
            # fit actual model to real data
            lm_actual = LinearRegression(n_jobs=int(1e4))
            lm_actual.fit(X=X, y=y)
            # fit second model to shuffle data
            lm_shuffle = LinearRegression(n_jobs=int(1e4))
            lm_shuffle.fit(X=np.random.permutation(X), y=y)

            rsm_copy = deepcopy(rsm_parts_animals[animal1]['rsm'][s])
            nan_mask = ~np.isnan(rsm_copy[np.eye(rsm_copy.shape[0]).astype(bool)])
            target = rsm_copy[nan_mask, :][:, nan_mask]
            target_labels = onehot.transform(rsm_parts_animals['cannon_labels'][nan_mask])
            df_animals.iloc[c] = np.hstack((a1, s, lm_actual.score(target_labels, target), 0))
            c += 1
            df_animals.iloc[c] = np.hstack((a1, s, lm_shuffle.score(target_labels, target), 1))
            c += 1
    df_animals['Data'][df_animals['Data'] == 0] = "Actual"
    df_animals['Data'][df_animals['Data'] == 1] = "Shuffle"
    cols = ["Animal", "Sequence1", "Sequence2", "Decoding Accuracy", "Data"]
    df_sequences = pd.DataFrame(data=np.zeros([2 * (np.prod(rsm_parts_ordered.shape[:2]) - 1), len(cols)]),
                                columns=cols)
    # fit label encoder to 'cannon labels'
    onehot = OneHotEncoder(sparse_output=False)
    onehot.fit(rsm_parts_animals['cannon_labels'])
    # fit a linear model to rsm from all animals, and predict rsm left out
    c = 0
    for a, animal in enumerate(animals):
        n_seq = rsm_parts_animals[animal]['rsm'].shape[0]
        for s1 in range(n_seq):
            X = []
            y = []
            for s2 in range(n_seq):
                # if not target sequence, add rsm to fit data
                if s1 != s2:
                    rsm_copy = deepcopy(rsm_parts_animals[animal]['rsm'][s2])
                    nan_mask = ~np.isnan(rsm_copy[np.eye(rsm_copy.shape[0]).astype(bool)])
                    rsm_copy = rsm_copy[nan_mask, :][:, nan_mask]
                    y.append(rsm_copy)
                    X.append(onehot.transform(rsm_parts_animals['cannon_labels'][nan_mask]))
            # change from list into array
            X, y = np.array(X), np.array(y)
            if n_seq >= 2:
                X, y = X.reshape(-1, X.shape[-1]), y.reshape(-1, y.shape[-1])
            # fit actual model to real data
            lm_actual = LinearRegression(n_jobs=int(1e4))
            lm_actual.fit(X=X, y=y)
            # fit second model to shuffle data
            lm_shuffle = LinearRegression(n_jobs=int(1e4))
            lm_shuffle.fit(X=np.random.permutation(X), y=y)
            rsm_copy = deepcopy(rsm_parts_animals[animal]['rsm'][s1])
            nan_mask = ~np.isnan(rsm_copy[np.eye(rsm_copy.shape[0]).astype(bool)])
            target = rsm_copy[nan_mask, :][:, nan_mask]
            target_labels = onehot.transform(rsm_parts_animals['cannon_labels'][nan_mask])
            df_sequences.iloc[c] = np.hstack((a, s1, s2, lm_actual.score(target_labels, target), 0))
            c += 1
            df_sequences.iloc[c] = np.hstack((a, s1, s2, lm_shuffle.score(target_labels, target), 1))
            c += 1
    df_sequences['Data'][df_sequences['Data'] == 0] = "Actual"
    df_sequences['Data'][df_sequences['Data'] == 1] = "Shuffle"
    return df_animal_similarity, df_animal_ID, df_animals, df_sequences


def get_partitioned_rsm_similarity_resampled(animals, rsm_parts_animals, n_samples, n_reps=10):
    """
    Calculate partitioned event rate map similarity from resampling with varied numbers of cells.
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param rsm_parts_animals: partitioned similarities across animals, computed with get_rsm_partitioned_sequences
    :param n_samples: number of cells to include in resampling
    :param n_reps: number of times to repeat resampling procedure
    :return: dataframe of similarities across animals with varied numbers of cells
    """
    # Initialize downsampled matrix for comparisons
    rsm_fits_dscells = np.zeros([len(n_samples) * len(animals) ** 2 * n_reps, 5]) * np.nan
    c = 0
    for k in range(n_reps):
        for n in tqdm(n_samples, desc="Fitting rsm across animals with downsampling of cell numbers",
                      position=0, leave=True):
            for a1, animal1 in enumerate(animals):
                for a2, animal2 in enumerate(animals):
                    a1_n_cells, a2_n_cells = (rsm_parts_animals[animal1]["rsm"].shape[-1],
                                              rsm_parts_animals[animal2]["rsm"].shape[-1])
                    cell_idx1, cell_idx2 = (np.random.choice(np.arange(a1_n_cells), n, replace=True),
                                            np.random.choice(np.arange(a2_n_cells), n, replace=True))
                    rsm_a1, rsm_a2 = (rsm_parts_animals[animal1]["rsm"][:, :, :, cell_idx1],
                                      rsm_parts_animals[animal2]["rsm"][:, :, :, cell_idx2])
                    rsm_a1, rsm_a2 = (np.nanmean(np.nanmean(rsm_a1, axis=0), axis=-1),
                                      np.nanmean(np.nanmean(rsm_a2, axis=0), axis=-1))
                    rsm_a1, rsm_a2 = (rsm_a1[np.tri(rsm_a1.shape[0], k=-1).astype(bool)],
                                      rsm_a2[np.tri(rsm_a2.shape[0], k=-1).astype(bool)])
                    nan_mask = ~np.isnan(rsm_a1 + rsm_a2)
                    r = pearsonr(rsm_a1[nan_mask], rsm_a2[nan_mask])[0]
                    rsm_fits_dscells[c, :] = np.hstack((a1, a2, n, k, r))
                    c += 1
    df = pd.DataFrame(data=rsm_fits_dscells, columns=["Animal 1", "Animal 2", "N cells", "Rep", "Tau"])
    df.groupby("N cells").mean()
    return df


def get_noise_margin(rsm_parts_ordered):
    """
    Calculate the upper and lower bounds of the noise ceiling from similarities measured in true dataset
    :param rsm_parts_ordered: paritioned similairites across animals, ordered to match across (based on first animal)
    :return: noise margin - upper and lower bounds of noise ceiling for each animal; mask - values used for comparison
    """
    n_seq, n_animals, n_pixels = rsm_parts_ordered.shape[:3]
    # calculate the noise ceiling
    noise_margin = np.zeros([n_animals, 2])
    for a in range(n_animals):
            target_rsm = np.nanmean(rsm_parts_ordered[:, a], 0)
            rsm_ciel = np.nanmean(np.nanmean(rsm_parts_ordered, 0), 0)
            rsm_floor = deepcopy(rsm_parts_ordered)
            rsm_floor[:, a] = np.nan
            rsm_floor = np.nanmean(np.nanmean(rsm_floor, 0), 0)
            mask = ~np.isnan(target_rsm)
            mask[np.tri(mask.shape[0], k=1).astype(bool)] = False
            noise_margin[a, :] = (kendalltau(target_rsm[mask], rsm_floor[mask])[0],
                                  kendalltau(target_rsm[mask], rsm_ciel[mask])[0])
    return noise_margin, mask


def get_rsm_fit_bootstrap(rsm1, rsm2, n_deviations=100, n_shuffles=1000):
    """
    Calculate bootstrapped similarity between average rsm and target rsm
    :param rsm1: first rsm to calculate similarity (e.g., calculated from data)
    :param rsm2: second rsm to calculate similarity against (e.g., model result)
    :param n_deviations: number of times to iterate to calculate deviation from rsm comparison
    :param n_shuffles: number of times to perform shuffling to compute p-values
    :return: true correlation values, standard errors, and p-values of rsm similarity
    """
    # takes average rsm result of target rsm (actual data or model) and compares to average result of model
    # take lower triangle of each
    rsm1, rsm2 = rsm1[np.tri(rsm1.shape[0], k=-1).astype(bool)], rsm2[np.tri(rsm2.shape[0], k=-1).astype(bool)]
    # generate mask to remove nans
    nan_mask = ~np.isnan(rsm1 + rsm2)
    # remove nans
    rsm1, rsm2 = rsm1[nan_mask], rsm2[nan_mask]
    # calculate actual correlation
    true_corr = kendalltau(rsm1, rsm2)[0]
    # calculate standard error for each comparison using bootstrap procedure
    se = 0
    for i in range(n_deviations):
        # create idx for random draws with replacement from rsms
        choice_idx = np.sort(np.random.choice(np.arange(rsm1.shape[0]), size=int(rsm1.shape[0]),
                                                  replace=True))
        # calculate deviation from true correlation
        se += (true_corr - kendalltau(rsm1[choice_idx], rsm2[choice_idx])[0]) ** 2
    # compute SE from deviations
    se = np.sqrt(se / (n_deviations - 1))
    # calculate shuffled correlations to estimate p value
    shuffle_corr = np.zeros(n_shuffles) * np.nan
    for i in range(n_shuffles):
        shuffle_idx = np.random.permutation(rsm2.shape[0])
        shuffle_corr[i] = kendalltau(rsm1, rsm2[shuffle_idx])[0]
    p_val = (n_shuffles - (true_corr > shuffle_corr).sum()) / n_shuffles
    return true_corr, se, p_val


def get_rsm_fit_bootstrap_verbose(rsm1, rsm2, n_deviations=100):
    """
    Calculate bootstrapped similarity between average rsm and target rsm but return statistics from kendall tau
    :param rsm1: first rsm to calculate similarity (e.g., calculated from data)
    :param rsm2: second rsm to calculate similarity against (e.g., model result)
    :param n_deviations: number of times to iterate to calculate deviation from rsm comparison
    :return: dataframe with bootstrapped similarity comparisons and stats
    """
    # takes average rsm result of target rsm (actual data or model) and compares to average result of model
    # take lower triangle of each
    rsm1, rsm2 = rsm1[np.tri(rsm1.shape[0], k=-1).astype(bool)], rsm2[np.tri(rsm2.shape[0], k=-1).astype(bool)]
    # generate mask to remove nans
    nan_mask = ~np.isnan(rsm1 + rsm2)
    # remove nans
    rsm1, rsm2 = rsm1[nan_mask], rsm2[nan_mask]
    # calculate standard error for each comparison using bootstrap procedure
    boot_fits = np.zeros(n_deviations)
    for i in range(n_deviations):
        # create idx for random draws with replacement from rsms
        choice_idx = np.sort(np.random.choice(np.arange(rsm1.shape[0]), size=int(rsm1.shape[0]),
                                                  replace=True))
        # calculate deviation from true correlation
        boot_fits[i] += kendalltau(rsm1[choice_idx], rsm2[choice_idx])[0]
    return boot_fits


def get_ca1_model_fits(rsm_parts_averaged, rsm_models, feature_types):
    """
    Wrapper for get_rsm_fit_bootstrap to calculate model fits to dataset and organize into dataframe
    :param rsm_parts_averaged: averaged rsm result across animals and sequences from ca1 dataset
    :param rsm_models: model rsms stacked to fit against ca1 representation
    :param feature_types: list of feature types stacked in rsm_models (e.g., "PC", "GC2PC", etc...).
    :return: dataframe with bootstrapped similarity comparisons between each model and ca1 data
    """
    cols = ["Model", "Fit", "SE", "P value"]
    df_agg_bootstrap = pd.DataFrame(data=np.zeros([len(feature_types), len(cols)]) * np.nan, columns=cols)
    for f, feature_type in tqdm(enumerate(feature_types),
                                desc='Fitting models to aggregate data with bootstrap procedure',
                                position=0, leave=True):
        fit, se, p_val = get_rsm_fit_bootstrap(rsm_parts_averaged,
                                               rsm_models[feature_type]['averaged'])
        df_agg_bootstrap.iloc[f] = np.hstack((feature_type, fit, se, p_val))
    df_agg_bootstrap.iloc[:, 1:] = df_agg_bootstrap.iloc[:, 1:].astype(float)
    # Measure one-way anova of model effects
    cols = ["Model", "Fits"]
    n_bootstrap = 100
    df_agg_bootstrap_ANOVA = pd.DataFrame(data=np.zeros([len(feature_types) * n_bootstrap, len(cols)]), columns=cols)
    c = 0
    for f, feature_type in tqdm(enumerate(feature_types),
                                desc='Fitting models to aggregate data with bootstrap procedure',
                                position=0, leave=True):
        fits = get_rsm_fit_bootstrap_verbose(rsm_parts_averaged, rsm_models[feature_type]['averaged'], n_bootstrap)
        df_agg_bootstrap_ANOVA.iloc[c:c+n_bootstrap, :] = np.vstack((np.tile(f, n_bootstrap), fits)).T
        c += n_bootstrap
    df_agg_bootstrap_ANOVA.iloc[:, 1] = df_agg_bootstrap_ANOVA.iloc[:, 1].astype(float)
    formula = 'Fits ~ C(Model)'
    lm = ols(formula, df_agg_bootstrap_ANOVA).fit()
    print(f"One-way ANOVA for effect of model on predicting CA1 representation: \n{anova_lm(lm)}")
    return df_agg_bootstrap


def get_ca1_model_fits_sequences(rsm_parts_ordered, rsm_models, feature_types, feature_names):
    """
    Wrapper for get_rsm_fit_bootstrap to calculate model fits to dataset across geometric sequences and organize into
    dataframe
    :param rsm_parts_ordered: ordered rsm result across animals and sequences from ca1 dataset
    :param rsm_models: model rsms stacked to fit against ca1 representation
    :param feature_types: list of feature types stacked in rsm_models (e.g., "PC", "GC2PC", etc...).
    :param feature_names: names of features used for subsequent plotting
    :return: dataframe with bootstrapped similarity comparisons between each model and ca1 data
    """
    rsm_parts_sequences = np.nanmean(rsm_parts_ordered, axis=1)
    n_seq = rsm_parts_sequences.shape[0]
    cols = ["Model", "Fit", "Sequence", "SE"]
    df_agg_bootstrap = pd.DataFrame(data=np.zeros([len(feature_types) * n_seq, len(cols)]) * np.nan, columns=cols)
    count = 0
    for f, feature_type in tqdm(enumerate(feature_types),
                                desc='Fitting models to aggregate data with bootstrap procedure',
                                position=0, leave=True):
        for s in range(n_seq):
            fit, se, p_val = get_rsm_fit_bootstrap(rsm_parts_sequences[s],
                                                   rsm_models[feature_type]['averaged'])
            df_agg_bootstrap.iloc[count] = np.hstack((feature_names[f], fit, s, se))
            count += 1
    df_agg_bootstrap.iloc[:, 1:] = df_agg_bootstrap.iloc[:, 1:].astype(float)
    # verbose bootstrap for one-way anova of model effects
    cols = ["Model", "Sequence", "Fits"]
    n_bootstrap = 100
    df_agg_bootstrap_ANOVA = pd.DataFrame(data=np.zeros([len(feature_types) * n_bootstrap * n_seq, len(cols)]),
                                          columns=cols)
    c = 0
    for f, feature_type in tqdm(enumerate(feature_types),
                                desc='Fitting models to aggregate data with bootstrap procedure',
                                position=0, leave=True):
        for s in range(n_seq):
            fits = get_rsm_fit_bootstrap_verbose(rsm_parts_sequences[s], rsm_models[feature_type]['averaged'],
                                                 n_bootstrap)

            df_agg_bootstrap_ANOVA.iloc[c:c + n_bootstrap, :] = np.vstack(
                (np.tile(f, n_bootstrap), np.tile(s, n_bootstrap),
                 fits)).T
            c += n_bootstrap
    df_agg_bootstrap_ANOVA.iloc[:, 1:] = df_agg_bootstrap_ANOVA.iloc[:, 1:].astype(float)
    formula = 'Fits ~ C(Model) + C(Sequence) + C(Model):C(Sequence)'
    lm = ols(formula, df_agg_bootstrap_ANOVA).fit()
    print(f"Two-way ANOVA for effect of model on predicting CA1 representation across sequences: \n{anova_lm(lm)}")
    return df_agg_bootstrap, n_seq


def get_ca1_model_fit_subsets(rsm_parts_animals, rsm_parts_averaged, rsm_models, feature_types, feature_names):
    """
    Compute CA1 model fits for specific subsets of comparisons - different environment same partition, same environment
    different partition, and different environment different partition.
    :param rsm_parts_animals: rsms computed from ca1 dataset for all animals and sequences
    :param rsm_parts_averaged: average resulting rsm across animals and sequences
    :param rsm_models: stacked rsms computed from each model
    :param feature_types: types of features in each model (e.g., "PC", "GC2PC_th", etc...).
    :param feature_names: names for plotting and stats with each model feature (e.g., "PC", "GC2PC", etc...).
    :return: dataframe of CA1 model fits for each subset of comparisons
    """
    # create boolean masks for matching and nonmatching partitions allocentrically using cannon labels
    cannon_labels = rsm_parts_animals["cannon_labels"]
    parts_mesh = np.meshgrid(cannon_labels[:, 1].astype(float), cannon_labels[:, 1].astype(float))
    parts_match = parts_mesh[0] == parts_mesh[1]
    parts_nonmatch = parts_mesh[0] != parts_mesh[1]
    # Break down non-match to same and different environments
    same_env_mesh = np.meshgrid(cannon_labels[:, 0], cannon_labels[:, 0])
    parts_nonmatch_senv = same_env_mesh[0] == same_env_mesh[1]
    parts_nonmatch_senv[:9, 90:] = False
    parts_nonmatch_senv[90:, :9] = False
    parts_nonmatch_senv[np.eye(parts_nonmatch_senv.shape[0]).astype(bool)] = False
    parts_nonmatch_denv = deepcopy(parts_nonmatch)
    parts_nonmatch_denv[parts_nonmatch_senv] = False
    parts_match_denv = deepcopy(parts_match)
    parts_match_denv[np.eye(parts_match.shape[0]).astype(bool)] = False
    # compare parts_nonmatch_senv (diff parts, same env), parts_match_denv (diff env, same parts), and parts_nonmatch_
    # denv (diff env, diff parts) to each of the hypotheses
    rsm_mask = ~np.isnan(rsm_parts_averaged)
    rsm_mask[np.tri(rsm_mask.shape[0], k=0).astype(bool)] = False

    # how well does each hypothesis explain different partitions, same environment?
    comp_masks = np.stack((np.logical_and(rsm_mask, parts_nonmatch_senv), np.logical_and(rsm_mask, parts_match_denv),
                           np.logical_and(rsm_mask, parts_nonmatch_denv)))
    cols = ["Model", "Comparison", "Tau", "SE", "P value"]
    df_hypo_comps = pd.DataFrame(data=np.zeros([comp_masks.shape[0] * len(feature_types), len(cols)]), columns=cols)
    c = 0
    for m, mask in enumerate(comp_masks):
        temp = deepcopy(rsm_parts_averaged)
        temp[~mask] = np.nan
        for f, feature_type in enumerate(feature_types):
            temp_model = deepcopy(rsm_models[feature_type]['averaged'])
            temp_model[~mask] = np.nan
            fit, se, p_val = get_rsm_fit_bootstrap(temp.T, temp_model.T)
            if m == 0:
                comp_name = "SE-DP"
            elif m == 1:
                comp_name = "DE-SP"
            elif m == 2:
                comp_name = "DE-DP"
            df_hypo_comps.iloc[c] = np.hstack((feature_names[f], comp_name, fit, se, p_val))
            c += 1
    df_hypo_comps["Tau"] = df_hypo_comps["Tau"].astype(float)
    df_hypo_comps["SE"] = df_hypo_comps["SE"].astype(float)

    n_bootstrap = 100
    cols = ["Model", "Comparison", "Tau"]
    df_hypo_comps_verbose = pd.DataFrame(data=np.zeros([comp_masks.shape[0] * len(feature_types) * n_bootstrap, len(cols)]),
                                         columns=cols)
    c = 0
    for m, mask in enumerate(comp_masks):
        temp = deepcopy(rsm_parts_averaged)
        temp[~mask] = np.nan
        for f, feature_type in enumerate(feature_types):
            temp_model = deepcopy(rsm_models[feature_type]['averaged'])
            temp_model[~mask] = np.nan
            fits = get_rsm_fit_bootstrap_verbose(temp.T, temp_model.T,
                                                 n_bootstrap)
            if m == 0:
                comp_name = "SE-DP"
            elif m == 1:
                comp_name = "DE-SP"
            elif m == 2:
                comp_name = "DE-DP"
            df_hypo_comps_verbose.iloc[c:c + n_bootstrap] = np.vstack((np.tile(feature_names[f], n_bootstrap),
                                                               np.tile(comp_name, n_bootstrap), fits)).T
            c += n_bootstrap
    df_hypo_comps_verbose["Tau"] = df_hypo_comps_verbose["Tau"].astype(float)
    formula = 'Tau ~ C(Model) + C(Comparison) + C(Model):C(Comparison)'
    lm = ols(formula, df_hypo_comps_verbose).fit()
    print(f"Two-way ANOVA for effect of model and comparison type on predicting CA1 representation: \n{anova_lm(lm)}")
    return df_hypo_comps


def get_rsm_partitioned_sequences_models(animals, p, file_ext='rsm_partitioned', agg=True):
    '''

    Generate partitioned similairity measures (averaged over cells) by sequence for model results across animals and
    store in nested dictionary along with labels and sorted in cannon order (defined by first animal's geometric
     sequence).
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param file_ext: file name extension for saved results from model rsm dictionary
    :return: rsm_animals - dictionary of partitioned similarity for all animals
    :arg agg: option to aggregate similarity across all cells (True), or calculate for each model cell / feature (False)
    :return: rsm_animals - dictionary of partitioned similarity for all animnals for all models
    '''
    os.chdir(p)
    rsm_animals = {}
    for animal in animals:
        os.chdir(os.path.join(p))
        # load the cell- and partion-wise RSM previously calculated for each animal
        rsm_dict = joblib.load(f'{animal}_{file_ext}')
        # average across cell axis (last) if not already using agg (PV corr) rsm
        if agg:
            rsm_average = np.nanmean(rsm_dict['RSM'], -1)
        else:
            rsm_average = rsm_dict['RSM']
        # get number of days, partitions, shapes, and sequences
        n_days = rsm_dict['envs'].shape[0]
        n_parts = rsm_average.shape[0] // n_days
        n_shapes = np.unique(rsm_dict['envs']).shape[0]
        n_seq = n_days // n_shapes
        if not agg:
            n_cells = rsm_average.shape[-1]
        # if first animal get 'cannon' labels for sorting other animals partitioned sequences
        if animal == animals[0]:
            rsm_animals['cannon_labels'] = np.vstack((np.tile(rsm_dict['envs'][np.newaxis].T, n_parts).ravel(),
                                                      rsm_dict['p_labels'])).T
        # target lables are those for actual animal under consideration
        target_labels = np.vstack((np.tile(rsm_dict['envs'][np.newaxis].T, n_parts).ravel(),
                                   rsm_dict['p_labels'])).T
        # get start and stop idx for deformed shapes (since squares will stay in place)
        def_idx = (n_parts, n_shapes * n_parts)
        # create sorting index for target rsm relative to cannon that can be applied to each sequence separately
        sort_idx = np.array([np.where(np.all(rsm_animals['cannon_labels'][e] == target_labels[def_idx[0]:def_idx[1]],
                                             axis=1))[0]
                             for e in range(def_idx[0], def_idx[1])]).ravel() + n_parts
        # concatenate on square indices now that deformed shapes are given indices
        sort_idx = np.hstack((np.arange(0, n_parts), sort_idx, np.arange(n_shapes * n_parts, (n_shapes + 1) * n_parts)))
        # sorted rsm will be number of partitions in each sequence (square to square) ** 2 by the number of sequences
        if agg:
            rsm_animals[animal] = np.zeros([n_seq, (n_shapes + 1) * n_parts, (n_shapes + 1) * n_parts]) * np.nan
        else:
            rsm_animals[animal] = np.zeros([n_seq, (n_shapes + 1) * n_parts, (n_shapes + 1) * n_parts, n_cells]) * np.nan
        square_idx = np.where(rsm_dict['envs'] == 'square')[0] * n_parts
        for i in range(square_idx[:-1].shape[0]):
            j, k = square_idx[i:i+2]
            k += n_parts
            rsm_animals[animal][i, :, :] = rsm_average[j:k, j:k][sort_idx, :][:, sort_idx]
        # if model dict has "constant" or "scalar" (params for BVCs) add to the dictionary for specific animal
        if "constant" in list(rsm_dict.keys()) and "scalar" in list(rsm_dict.keys()):
            rsm_animals["constant"], rsm_animals["scalar"] = rsm_dict["constant"], rsm_dict["scalar"]
        if "beta_a" in list(rsm_dict.keys()) and "beta_b" in list(rsm_dict.keys()):
            rsm_animals["beta_a"], rsm_animals["beta_b"] = rsm_dict["beta_a"], rsm_dict["beta_b"]
    print('Loaded model RSMs for all animals')
    rsm_animals['cannon_labels'] = rsm_animals['cannon_labels'][:n_parts * 11]
    return rsm_animals


def get_rsm_partitioned_similarity_models(rsm_parts_models, animals, get_sequence_similarity=True,
                                          get_animal_similarity=True):
    '''
    Calculate the similarity of partitioned rsms within animals across sequences and across animals within the same
    sequences for model results.
    :param rsm_parts_models: dictionary of rsm paritioned similarity results from get_rsm_partitioned_sequences_models
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :return: sequence_similarity - dataframe of similarities across seqeunce (optional) for model results;
    animal_similarity - dataframe of similarities across animals (optional) for model results;
    rsm_parts_ordered - partiioned similarities across geometries ordered to match across animals, rsm_parts_averaged -
    average results across animals and sequences
    '''
    rsm_parts_ordered = np.nan * np.zeros([rsm_parts_models[animals[0]].shape[0], len(animals),
                                           rsm_parts_models[animals[0]].shape[1],
                                           rsm_parts_models[animals[0]].shape[2]])
    for a, animal in enumerate(animals):
        n_seq = rsm_parts_models[animal].shape[0]
        for s in range(n_seq):
            rsm_parts_ordered[s, a] = rsm_parts_models[animal][s]
    if get_sequence_similarity:
        sequence_similarity = calculate_sequence_similarity(rsm_parts_ordered)
    if get_animal_similarity:
        animal_similarity = calculate_animal_similarity(rsm_parts_ordered)
    rsm_parts_averaged = np.nanmean(np.nanmean(rsm_parts_ordered, 0), 0)
    if get_sequence_similarity and get_animal_similarity:
        return sequence_similarity, animal_similarity, rsm_parts_ordered, rsm_parts_averaged
    else:
        return rsm_parts_ordered, rsm_parts_averaged


def get_rsm_model_dict(animals, feature_types, p_models):
    """
    Build dictionary of model results for comparisons against actual data across all animals
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param feature_types: list of models feature types as string (e.g., ["PC", "GC2PC", etc...]
    :param p_models: path to model results (e.g., "User/yourname/Desktop/georepca1/results/riab")
    :return:
    """
    # creates a dictionary for each model rsm averaged across sequences and ordered for each animal and sequence
    rsm_models = {}
    for feature_type in feature_types:
        rsm_parts_model = get_rsm_partitioned_sequences_models(animals, p_models,
                                                               file_ext=f'model_{feature_type}_rsm_partitioned_cellwise',
                                                               agg=True)
        rsm_models[feature_type] = {}
        rsm_models[feature_type]['ordered'], rsm_models[feature_type]['averaged'] = \
            get_rsm_partitioned_similarity_models(rsm_parts_model, animals, get_sequence_similarity=False,
                                           get_animal_similarity=False)
    os.chdir(p_models)
    joblib.dump(rsm_models, 'rsm_models')
    print(f"Model rsm dictionary created and saved in {p_models}")


def get_model_rsm(animals, p, feature_type):
    """
    Wrapper functioun to generate a model rsm dictionary with get_cell_rsm_paritioned
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param feature_type: name of feature type as string (e.g., "GC2PC")
    :return: dictionary of model results across animals
    """
    # load in results from simulation
    p_models = os.path.join(p, "results", "riab")
    # load in actual behavioral data
    behav_dict = joblib.load(os.path.join(p, "data", "behav_dict"))
    for animal in animals:
        # construct model event rate maps
        models_maps = joblib.load(os.path.join(p_models, f'{animal}_{feature_type}_maps'))
        # build rsm from simulated event rate maps
        rsm_model, rsm_labels_model, cell_idx_model = get_cell_rsm_partitioned(models_maps)
        # add into dictionary with structured information including relevant info like environment, day, partition, etc.
        rsm_dict_model = {'RSM': rsm_model, 'd_labels': rsm_labels_model[:, 0], 'p_labels': rsm_labels_model[:, 1],
                          'cell_idx': cell_idx_model, 'envs': behav_dict[animal]['envs']}
        # save results and return dictionary
        joblib.dump(rsm_dict_model, os.path.join(p_models, f'{animal}_model_{feature_type}_rsm_partitioned_cellwise'))
    return rsm_dict_model


########################################################################################################################
# Bayesian decoding of animal position within sessions
def fit_decoder(behav, traces, feature_max, temporal_bin_size=3):
    """
    Fit naive Bayes decoder to temporally binned behavioral and calcium trace data.
    :param behav: x-y position of animal shape n frames (row) by n features (x and y position in columns)
    :param traces: calcium trace data shape n frames (row) by n cells (column)
    :param feature_max: maximum value for behavioral features (environment size)
    :param temporal_bin_size: number of frames to average in temporal binning
    :return: naive Bayes model fit to data
    """
    # temporally bin trace and behavioral data
    pooling = AvgPool1d(kernel_size=temporal_bin_size, stride=temporal_bin_size)
    behav, traces = pooling(torch.tensor(behav.T)).numpy().astype(int).T, \
        pooling(torch.tensor(gaussian_filter1d(traces, sigma=temporal_bin_size, axis=0).T)).numpy().T
    # determine number of samples (time points) and behavioral features
    n_samples, n_features = behav.shape[0], behav.shape[1]
    # transform behavioral data to one-hot embedding
    onehot_behav = np.zeros(n_samples)
    empty_map = np.zeros(feature_max)
    for i in range(n_samples):
        empty_map[behav[i][0], behav[i][1]] += 1
        onehot_behav[i] = np.where(empty_map.flatten())[0]
        empty_map *= 0
    # initialize Gaussian naive Bayes model with flat priors
    n_classes = np.unique(onehot_behav).shape[0]
    model = GaussianNB(priors=np.ones(n_classes)/n_classes)
    # fit the model to temporally binned dataset
    model.fit(traces, onehot_behav)
    return model


def test_decoder(model, behav, traces, feature_max, bin_down, temporal_bin_size=3):
    """
    Test (predict) behavioral data from calcium traces with fit naive Bayes decoding model
    :param model: fit naive Bayes model
    :param behav: x-y position of animal shape n frames (row) by n features (x and y position in columns)
    :param traces: calcium trace data shape n frames (row) by n cells (column)
    :param feature_max: maximum value for behavioral features (environment size)
    :param bin_down: spatial bin size for x and y position
    :param temporal_bin_size: number of frames to average in temporal binning
    :return: behav - temporally binned behavioral data; predictions_transform - transformed predictions from onehot
    embedding; err_squared - squared distance between actual and predicted position; predictions_map - distance map of
    predicted and actual locations for x-y position
    """
    # temporally bin behav data
    pooling = AvgPool1d(kernel_size=temporal_bin_size, stride=temporal_bin_size)
    behav, traces = pooling(torch.tensor(behav.T)).numpy().astype(int).T,\
                    pooling(torch.tensor(gaussian_filter1d(traces, sigma=temporal_bin_size, axis=0).T)).numpy().T
    # make predictions of one-hot behavior using trained model
    predictions = model.predict(traces).astype(int)
    # also return probabilities for all classes from predictions (shape n_samples X n_classes)
    # reverse-transform predictions
    n_samples, n_features = behav.shape[0], behav.shape[1]
    predictions_transform = np.zeros([n_samples, n_features])
    empty_map = np.zeros(feature_max)
    predictions_map = np.zeros([feature_max[0], feature_max[1], feature_max[0], feature_max[1]])
    for i in range(n_samples):
        trans = empty_map.flatten()
        trans[predictions[i]] += 1
        empty_map[np.where(trans.reshape(empty_map.shape))] += 1
        predictions_map[behav[i][0], behav[i][1]] += empty_map/n_samples
        predictions_transform[i] = np.array(np.where(empty_map)).squeeze()
        empty_map *= 0
    # calculate the squared distance between actual and predicted behavior
    err_squared = np.zeros(n_samples)
    for i in range(n_samples):
        err_squared[i] = (bin_down * np.linalg.norm(predictions_transform[i] - behav[i])) ** 2
    return behav, predictions_transform, err_squared, predictions_map


def decode_position_within(behav, traces, maps, n_bins=15, fps=30, v_filt_size=5,
                           v_thresh=5, cell_threshold=5, buffer=1e-15, n_fold=5):
    """
    Decode animal temporal and spatially binned position with k-fold cross validation and naive Bayesian method.
    :param behav: x-y position of animal shape n frames (row) by n features (x and y position in columns)
    :param traces: calcium trace data shape n frames (row) by n cells (column)
    :param maps: pre-computed event rate maps from calcium and position data
    :param n_bins: number of spatial bins for x and y position
    :param fps: frames per second of original data
    :param v_filt_size: size of temporal window of filter used for velocity estimates
    :param v_thresh: threshold for velocity to be included in fitting and predictions
    :param cell_threshold: threshold for activity sparsity of cells to be included in fitting and predictions
    :param buffer: buffer size for rounding of position
    :param n_fold: number of cross validation folds
    :return: distances - Euclidean error in position decoding at each binned time point, np.sqrt(mse) - spatial map of
    average Euclidean error between actual and decoded position, imse - inverse mse, coherence_maps - coherence map of
    decoding errors
    """
    # make a deepcopy of behav and trace data as to not change original inputs
    behav = deepcopy(behav)
    traces = deepcopy(traces)
    # bin down behavioural data
    behav_max = behav.max(axis=0).max(axis=1)
    bin_down = (behav_max.max() + buffer) / n_bins
    behav /= bin_down
    # determine maxima of behavioral features across all days (needed for across-day training and decoding)
    behav_max = (behav.max(axis=0).max(axis=1) + buffer).astype(int)
    # grab the number of days to iterate across, and initialize some variables to return and use as function params
    n_days = behav.shape[2]
    n_cells = traces.shape[1]
    # distances will contain the distance errors for model predictions
    distances = np.zeros([n_days, n_fold])
    # initialize velocity and cell idx for selecting data based on criteria
    cell_idx = np.zeros([n_days, n_cells]).astype(bool)
    vel_idx = np.zeros([n_days, behav.shape[0]]).astype(bool)
    # create template maps for all days to find nearest x-y bins for dist_maps after predictions are made (cleaning)
    temp_maps = np.nansum(maps, axis=2).astype(bool)
    # initialize distance maps to show distance between actual and decoded position in each bin
    mse, imse, coherence_maps = np.zeros_like(temp_maps).astype(float), np.zeros_like(temp_maps).astype(float),\
        np.zeros_like(temp_maps).astype(float)
    for d in tqdm(range(n_days), desc=f'Performing {n_fold}-fold cross-validated position decoding'):
        vel_idx[d, 1:] = gaussian_filter1d(np.linalg.norm((behav[1:, :, d] - behav[:-1, :, d]) * fps, axis=1),
                                           axis=0, sigma=v_filt_size) > (v_thresh / bin_down)
        cell_idx[d] = np.sum(traces[:, :, d][vel_idx[d]], axis=0) > cell_threshold
        target_behav = behav[:, :, d][vel_idx[d]]
        target_traces = traces[:, cell_idx[d], d][vel_idx[d]]
        kf = KFold(n_splits=n_fold)
        kf.get_n_splits(target_traces)
        for fold, (train_index, test_index) in enumerate(kf.split(target_traces)):
            # initialize maps to plot distance errors onto behavioural map
            count_map, err_squared_map, prob_map = np.zeros_like(temp_maps[:, :, d]).astype(int), \
                                                   np.zeros_like(temp_maps[:, :, d]).astype(int), \
                                                   np.zeros_like(temp_maps[:, :, d]).astype(int)
            # index target and train trace and behavioural data
            behav_train, behav_test = target_behav[train_index], target_behav[test_index]
            traces_train, traces_test = target_traces[train_index], target_traces[test_index]
            # fit the decoding model to the training data
            GNB_model = fit_decoder(behav_train, traces_train, behav_max)
            # apply the model on the test data and measure the accuracy of predictions
            # return the temporally binned behavioral data (actual), the predictions, and the distance as error metric
            actual, predictions, err_squared, predictions_dist = test_decoder(GNB_model, behav_test, traces_test,
                                                                              behav_max, bin_down)
            # clean the actual and predicted position data using temp_maps
            true_bins = np.array(np.argwhere(temp_maps[:, :, d]))
            for i in range(actual.shape[0]):
                actual_diff, pred_diff = true_bins - np.array(actual[i]), true_bins - np.array(predictions[i])
                actual_norms, pred_norms = np.linalg.norm(actual_diff, axis=1), np.linalg.norm(pred_diff, axis=1)
                actual[i], predictions[i] = true_bins[np.argmin(actual_norms)], true_bins[np.argmin(pred_norms)]
            distances[d, fold] = np.sqrt(err_squared).mean()
            for i, (x, y) in enumerate(actual):
                count_map[x, y] += 1
                err_squared_map[x, y] += err_squared[i]
            for x in range(predictions_dist.shape[0]):
                for y in range(predictions_dist.shape[1]):
                    predictions_dist[x, y] = (count_map + 1).sum() * predictions_dist[x, y] / (count_map + 1)
                    if np.any(predictions_dist[x, y].astype(bool)):
                        smooth_dist = gaussian_filter(predictions_dist[x, y], sigma=1.5)
                        r_, _ = pearsonr(smooth_dist.flatten(), predictions_dist[x, y].flatten())
                        coherence = .5 * np.log((1 + r_) / (1 - r_))
                        coherence_maps[x, y, d] = coherence
                    else:
                        coherence_maps[x, y, d] = np.nan
            mse[:, :, d] = err_squared_map / count_map
            imse[:, :, d] = 1 / err_squared_map
    return distances, np.sqrt(mse), imse, coherence_maps


########################################################################################################################
# Methods to build dataframes for plotting and stats from numpy arrays and nested dictionaries
def get_all_shr_pvals(animals, p):
    '''
    Generate dataframe for plotting and statistics with split-half reliability values for all cells across animals
    :param animals: list of animals included to collect pre-calculated p values
    :param p: path to pre-calculated p values
    :return: df_pvals - dataframe indicating split-half reliability p value for each cell and day
    '''
    p_results = os.path.join(p, "results")
    group_p_vals = {}
    animal_days = np.zeros(len(animals)) * np.nan
    # use cell num counter for later initialization of the p-vals mat across animals and days
    total_cells = 0
    for a, animal in enumerate(animals):
        group_p_vals[animal] = joblib.load(os.path.join(p_results, f'{animal}_shr'))
        total_cells += group_p_vals[animal].shape[0]
        animal_days[a] = group_p_vals[animal].shape[1]
    max_days = animal_days.max()
    p_vals = np.zeros([total_cells, int(max_days)]) * np.nan
    animal_id = np.zeros([total_cells, int(max_days)]) * np.nan
    idx = 0
    for a, animal in enumerate(animals):
        n_cells, n_days = group_p_vals[animal].shape
        p_vals[idx:idx + n_cells, :n_days] = group_p_vals[animal]
        animal_id[idx:idx + n_cells, :n_days] = a
        idx += n_cells
    cols = ['Animal', 'Day', 'Cell', 'SHR']
    df_pvals = pd.DataFrame(columns=cols, data=np.zeros([p_vals.ravel().shape[0], len(cols)])*np.nan)
    idx = 0
    for c in range(p_vals.shape[0]):
        for d in range(p_vals.shape[1]):
           df_pvals.iloc[idx] = pd.Series(np.array([animal_id[c, d], d, c, p_vals[c, d]]))
           idx += 1
    return df_pvals.dropna(axis=0)


def get_all_decoding_within(animals, p):
    """
    Generate dataframe for plotting and statistics of within-session decoding error for all animals and all sessions
    :param animals: list of animals included to collect pre-calculated decoding errors
    :param p: path to pre-calculated p values
    :return: df_decoding - dataframe containing average Euclidean decoding errors for animal position across days
    """
    p_results = os.path.join(p, "results")
    group_decoding = joblib.load(os.path.join(p_results, "within_decoding"))
    n_animals = len(list(group_decoding.keys()))
    max_days = max([group_decoding[animal]['decoding_error'].shape[0] for animal in list(group_decoding.keys())])
    decoding = np.zeros([n_animals, int(max_days)]) * np.nan
    for a, animal in enumerate(animals):
        n_days = group_decoding[animal]['decoding_error'].shape[0]
        decoding[a, :n_days] = group_decoding[animal]['decoding_error'].mean(1)
    cols = ['Day', 'Animal', 'Error']
    df_decoding = pd.DataFrame(columns=cols, data=np.zeros([decoding.ravel().shape[0], len(cols)])*np.nan)
    idx = 0
    for a in range(decoding.shape[0]):
        for d in range(decoding.shape[1]):
            df_decoding.iloc[idx] = pd.Series(np.array([d, a, decoding[a, d]]))
            idx += 1
    return df_decoding.dropna(axis=0)


########################################################################################################################
# Heuristic model simulations
def get_euclidean_similarity_partitioned(envs):
    """
    Simulate heuristic model of Euclidean similarity between all 3x3 partitions across pairs of environment geometries
    :param envs: list of enviornment geometries to compute similarity (e.g., "square", "o", "+", etc...).
    :return: normalized Euclidean distances between all partitions across all environment pairs
    """
    # represent every partition in the environment with a one in the two-dim matrix
    euc_parts = []
    for e, env in enumerate(envs):
        env_mat = get_env_mat(env)
        # orient the env mat the same as the rate maps for correctly ordering comparisons
        flat_env_mat = np.flipud(env_mat).T.ravel()
        for n, part in enumerate(flat_env_mat):
            this_part = np.zeros(flat_env_mat.shape[0])
            if part:
                this_part[n] = 1.
            else:
                this_part *= np.nan
            euc_parts.append(this_part.reshape(env_mat.shape[0], -1))
    # Similarity measured as the vector norm between pairwise locations
    euc_similarity = np.zeros([len(envs) * flat_env_mat.shape[0],
                               len(envs) * flat_env_mat.shape[0]]) * np.nan
    for i, part1 in enumerate(euc_parts):
        for j, part2 in enumerate(euc_parts):
            if np.all(~np.isnan(part1)) and np.all(~np.isnan(part2)):
                euc_similarity[i, j] = np.linalg.norm(np.abs(np.argwhere(part1) -
                                                             np.argwhere(part2)))
    # minmax normalize values
    euc_similarity = np.nanmax(euc_similarity) - euc_similarity
    euc_similarity -= np.nanmin(euc_similarity)
    euc_similarity /= np.nanmax(euc_similarity)
    return euc_similarity


def get_boundary_similarity_partitioned(envs):
    """
    Simulate heuristic model of hamming distances between all local boundary conditions between all 3x3 partitions
    across pairs of environment geometries
    :param envs: list of enviornment geometries to compute similarity (e.g., "square", "o", "+", etc...).
    :return: similarity of boundary conditions between all partitions across all environment pairs
    """
    # represent every partition in the environment with its boundary conditions (1 or 0)
    boundary_conditions = {"envs": envs, "bounds": []}
    for env in envs:
        env_mat = get_env_mat(env)
        # orient the env mat the same as the rate maps for correctly ordering comparisons
        flat_env_mat = np.flipud(env_mat).T.ravel()
        env_mat = np.flipud(get_env_mat(env)).T
        env_mat = ~np.pad(env_mat, 1, "constant").astype(bool)
        part_boundaries = np.zeros([3, 3, 4])
        for row in np.arange(1, 4):
            for col in np.arange(1, 4):
                if ~env_mat[row, col]:
                    north = env_mat[row + 1, col] # N boundary condition
                    west = env_mat[row, col - 1] # W boundary condition
                    south = env_mat[row - 1, col] # S boundary condition
                    east = env_mat[row, col + 1] # E boundary condition
                    part_boundaries[row-1, col-1, :] = np.array([north, west, south, east])
                else:
                    part_boundaries[row - 1, col - 1, :] = np.zeros(4) * np.nan
        boundary_conditions["bounds"].append([part_boundaries.reshape(-1, 4)])
    boundary_conditions["bounds"] = np.array(boundary_conditions["bounds"]).reshape(-1, 4)
    # similarity measured as the hamming distance between local boundary conditions in each partition
    bound_similarity = np.zeros([len(envs) * flat_env_mat.shape[0],
                                 len(envs) * flat_env_mat.shape[0]]) * np.nan
    for i, bound1 in enumerate(boundary_conditions["bounds"]):
        for j, bound2 in enumerate(boundary_conditions["bounds"]):
            if np.all(~np.isnan(bound1)) and np.all(~np.isnan(bound2)):
                bound_similarity[i, j] = 1 - hamming(bound1, bound2)
            else:
                bound_similarity[i, j] = np.nan
    return bound_similarity


def get_traj_similarity_partitioned(animals, envs, p):
    """
    Simulate heuristic model predictions for similarity of tranejectories across binned spatial locations of animals in
    each environment between all 3x3 partitions across pairs of geometries
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param envs: list of enviornment geometries to compute similarity (e.g., "square", "o", "+", etc...).
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :return: transition_similarity - trajectoral similarity calculated as the similarity of transition matrices between
    all 3x3 partitions across pairs of geometries
    """
    # first build out transition matrices for each animal
    transition_matrices = {}
    n_bins = 15
    step_size = 30
    # Note: exploratory analysis showed that step size at 1 frame does not fit as well as 1 or 2 sec (30-60 frames)
    for animal in animals:
        dat = load_dat(animal, p)
        transition_matrices[animal] = {"T": get_transition_matrix(dat[animal]["position"].T,
                                                                  dat[animal]["maps"]["smoothed"],
                                                                  n_bins=15,
                                                                  step_size=step_size)[0],
                                       "envs": dat[animal]["envs"]}
        del dat
    # re-order the transition matrices to follow cannon order (first animal), and break up by sequence
    for animal in animals:
        transition_matrices[animal]["T_shapes"] = np.zeros([envs.shape[0], n_bins ** 2, n_bins ** 2])
        for e, env in enumerate(envs):
            # pull out transition matrices for each shape following cannon order, and average across sequences
            env_idx = np.where(transition_matrices[animal]["envs"] == env)[0]
            transition_matrices[animal]["T_shapes"][e] = np.nanmean(transition_matrices[animal]["T"][:, :, env_idx], -1)
    # average across animals
    T_shapes = np.zeros([len(animals), len(envs), n_bins ** 2, n_bins ** 2])
    for a, animal in enumerate(animals):
        for e, env in enumerate(envs):
            T_shapes[a, e] = transition_matrices[animal]["T_shapes"][e]
    T_shapes = np.nanmean(T_shapes, axis=0).squeeze()
    # in the final step of formatting the transition matrix organize by partition
    part_labels = np.flipud(np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]).T)  # reordered to match rate map orientation
    pixel_labels = part_labels[:, :, np.newaxis].repeat(25, axis=2).reshape(3, 3, 5, 5).transpose(0, 2, 1, 3).reshape(
        15,
        15).ravel()

    # now iterate through each shape and pull out the target partition
    transition_parts = []
    for T_shape in T_shapes:
        for part in part_labels.ravel():
            temp = deepcopy(T_shape)
            vals = temp[pixel_labels == part, pixel_labels == part]
            transition_parts.append(vals)
    # compute similarity of partitioned transition matrices
    transition_similarity = np.zeros([99, 99]) * np.nan
    for i, p1 in enumerate(transition_parts):
        for j, p2 in enumerate(transition_parts):
            transition_similarity[i, j] = pearsonr(p1, p2)[0]
    return transition_similarity


########################################################################################################################
# RatInABox (riab) model simulations
def deform_environment(Env, shape):
    """
    Deform environment geometry by inserting walls to riab environment as in Lee et al. (2025) study
    :param Env: riab environment to deform by addition of walls
    :param shape: shape of environment as a string (e.g., "glenn", "o", etc...).
    :return: None (Env object is modified in-place)
    """
    # Add walls to riab environment to deform geometry as per experiment
    if shape == 'glenn':
        Env.add_wall([[.25, 0], [.25, .25]])
        Env.add_wall([[.25, .25], [0, .25]])
        Env.add_wall([[.5, .5], [.5, .75]])
        Env.add_wall([[.5, .5], [.75, .5]])
    elif shape == 'o':
        Env.add_wall([[.25, .25], [.25, .5]])
        Env.add_wall([[.25, .5], [.5, .5]])
        Env.add_wall([[.5, .5], [.5, .25]])
        Env.add_wall([[.5, .25], [.25, .25]])
    elif shape == 'bit donut':
        Env.add_wall([[0., 0.25], [0.25, 0.25]])
        Env.add_wall([[0.25, 0.25], [0.25, 0]])
        Env.add_wall([[0.25, 0.25], [0.25, 0.5]])
        Env.add_wall([[0.25, 0.5], [0.5, 0.5]])
        Env.add_wall([[0.5, 0.5], [0.5, 0.25]])
        Env.add_wall([[0.5, .25], [0.25, 0.25]])
    elif shape == 'u':
        Env.add_wall([[.25, .25], [.75, .25]])
        Env.add_wall([[.25, .25], [.25, .5]])
        Env.add_wall([[.25, .5], [.75, .5]])
    elif shape == '+':
        Env.add_wall([[.25, 0.], [.25, .25]])
        Env.add_wall([[.25, .25], [0., .25]])
        Env.add_wall([[0., 0.5], [.25, .5]])
        Env.add_wall([[.25, .5], [.25, .75]])
        Env.add_wall([[.5, .5], [.5, .75]])
        Env.add_wall([[.5, .5], [.75, .5]])
        Env.add_wall([[.5, .25], [.75, .25]])
        Env.add_wall([[.5, .25], [.5, 0.]])
    elif shape == 't':
        Env.add_wall([[0.00, 0.25], [0.25, 0.25]])
        Env.add_wall([[0.25, 0.25], [0.25, 0.75]])
        Env.add_wall([[0.50, 0.75], [0.50, 0.25]])
        Env.add_wall([[0.50, 0.25], [0.75, 0.25]])
    elif shape == 'l':
        Env.add_wall([[0.25, 0.], [0.25, .5]])
        Env.add_wall([[0.25, .5], [0.75, .5]])
    elif shape == 'i':
        Env.add_wall([[.25, .25], [0., .25]])
        Env.add_wall([[.25, .25], [.25, .5]])
        Env.add_wall([[.25, .5], [0., .5]])
        Env.add_wall([[.5, .25], [.75, .25]])
        Env.add_wall([[.5, .25], [.5, .5]])
        Env.add_wall([[.5, .5], [.75, .5]])
    elif shape == 'rectangle':
        Env.add_wall([[.25, .0], [0.25, .75]])


def simulate_bases(animals, p, n_features=200, field_width=0.15, grid_scale=(0.28, 0.73),
                   fps=30, env_size=75, bases=['GC', 'BVC'], random_seed=2023):
    """
    Simulate a "basis set" of spatial features following animal trajectories with riab toolbox. See toolbox for details:
    https://github.com/RatInABox-Lab/RatInABox
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param n_features: number of features to simulate for each basis set (i.e., number of GCs, BVCs, PCs).
    :param field_width: width of gaussian place fields to simulate in meters
    :param grid_scale: grid scale for simulated grid cells, default is logarithmic scale from Solstad et al. (2006)
    :param fps: frames per second of original dataset
    :param env_size: size of environment in cm
    :param bases: list of bases to simulate (e.g., ["GC", "BVC", "PC"])
    :param random_seed: random seed for reproducibility
    :return: None (simulated data are saved to p/results/riab folder)
    """
    p_data = os.path.join(p, "data")
    p_results = os.path.join(p, "results")
    p_models = os.path.join(p_results, 'riab')
    # Generate simulations of Euclidean place cells, grid cells, and boundary-vector cells
    # load all behavioral data in one shot to simulate spike data for each animal and session
    behav_dict = joblib.load(os.path.join(p_data, 'behav_dict'))
    if glob(p_models) == []:
        os.mkdir(p_models)
    os.chdir(p_models)
    for animal in animals:
        # minmax scale position to match riab environments in meters (0.75 m)
        position = env_size * 0.01 * (behav_dict[animal]['position'] / behav_dict[animal]['position'].max())
        envs = behav_dict[animal]['envs']
        n_days = position.shape[0]
        n_steps = position.shape[2]
        if 'PC' in bases:
            PC_dict = {'animal': animal,
                       'envs': envs,
                       'position': position,
                       'firing_rates': np.zeros([n_days, n_features, n_steps]) * np.nan}
        if 'GC' in bases:
            GC_dict = {'animal': animal,
                       'envs': envs,
                       'position': position,
                       'firing_rates': np.zeros([n_days, n_features, n_steps]) * np.nan}
        if 'BVC' in bases:
            BVC_dict = {'animal': animal,
                       'envs': envs,
                       'position': position,
                       'firing_rates': np.zeros([n_days, n_features, n_steps]) * np.nan}
        if "egoBVC" in bases:
            egoBVC_dict = {'animal': animal,
                           'envs': envs,
                           'position': position,
                           'firing_rates': np.zeros([n_days, n_features, n_steps]) * np.nan}
        if 'HDC' in bases:
            HDC_dict = {'animal': animal,
                       'envs': envs,
                       'position': position,
                       'firing_rates': np.zeros([n_days, n_features, n_steps]) * np.nan}
        for d, env_name in tqdm(enumerate(envs), desc='Simulating spike data across days',
                                position=0, leave=True):
            # Initialise environment
            Env = Environment(
                params={'aspect': 1,
                        'scale': .75,
                        'dimensionality': '2D'})
            # Deform geometry according to what animal experienced
            deform_environment(Env, env_name)
            # Add agent and neuron types
            if d == 0:
                Ag = Agent(Env, params={"dt": 1/fps})
                if 'PC' in bases:
                    # set random seed to preserve systematic randomness across functions
                    np.random.seed(random_seed)
                    PC = PlaceCells(Ag, params={'n': n_features,
                                                'description': 'gaussian_threshold',
                                                'widths': field_width,
                                                'wall_geometry': 'euclidean',
                                                'max_fr': 1,
                                                'min_fr': 0,
                                                'color': 'C2'})
                if 'GC' in bases:
                    # set random seed to preserve systematic randomness across functions
                    np.random.seed(random_seed)
                    # Add grid cells

                    GC = GridCells(Ag, params={"n": n_features,
                                               "gridscale_distribution": "logarithmic",
                                               "gridscale": grid_scale,
                                               "orientation_distribution": "uniform",
                                               "orientation": (0, 2 * np.pi),
                                               "phase_offset_distribution": "uniform",
                                               "phase_offset": (0, 2 * np.pi),  # degrees
                                               "description": "three_shifted_cosines",
                                               "min_fr": 0,
                                               "max_fr": 1,
                                               "name": "GridCells"})
                if 'BVC' in bases:
                    # set random seed to preserve systematic randomness across functions
                    np.random.seed(random_seed)
                    # Add boundary vector cells

                    BVC = BoundaryVectorCells(Ag, params={"n": n_features,
                                                          "reference_frame": "allocentric",
                                                          "tuning_distance_distribution": "uniform",
                                                          "tuning_distance": (0, 0.85),
                                                          "tuning_angle_distribution": "uniform",
                                                          "sigma_distance": (0.08, 12),
                                                          "sigma_angle": (11.25, 11.25),
                                                          "sigma_angle_distribution": "uniform",
                                                          "dtheta": 2,
                                                          "min_fr": 0,
                                                          "max_fr": 1,
                                                          "name": "BoundaryVectorCells",
                                                          "color": "C2"})
                if "egoBVC" in bases:
                    # set random seed to preserve systematic randomness across functions
                    np.random.seed(random_seed)
                    # Add egocentric boundary vector cells
                    egoBVC = BoundaryVectorCells(Ag, params={"n": n_features,
                                                          "reference_frame": "egocentric",
                                                          "tuning_distance_distribution": "uniform",
                                                          "tuning_distance": (0, 0.85),
                                                          "tuning_angle_distribution": "uniform",
                                                          "sigma_distance": (0.08, 12),
                                                          "sigma_angle": (11.25, 11.25),
                                                          "sigma_angle_distribution": "uniform",
                                                          "dtheta": 2,
                                                          "min_fr": 0,
                                                          "max_fr": 1,
                                                          "name": "BoundaryVectorCells",
                                                          "color": "C2"})
                if 'HDC' in bases:
                    # set random seed to preserve systematic randomness across functions
                    np.random.seed(random_seed)
                    # Add head direction cells
                    HDC = HeadDirectionCells(Ag,
                                             params={"n": n_features,
                                                     "min_fr": 0,
                                                     "max_fr": 1,
                                                     "angular_spread_degrees": 45,
                                                     "name": "HeadDirectionCells"})
            else:
                Ag.Environment = Env

            Ag.import_trajectory(times=[i / fps for i in range(position.shape[-1])],
                                 positions=position[d].T,
                                 interpolate=False)
            # history is not imported with import trajectory, and needs to be initialized
            for key in list(Ag.history.keys()):
                Ag.history[key] = [0]
            # Simulate
            T = position.shape[-1]
            # update first time step with actual data
            for i in range(int(1)):
                Ag.update()
            # then drop the zeros that was initialized with
            for key in list(Ag.history.keys()):
                Ag.history[key] = Ag.history[key][1:]
            # proceed with actual updates for entire session
            # Simulate
            for i in tqdm(range(1, int(T)), leave=True, position=0, desc='Stepping through updates'):
                Ag.update()
                if 'PC' in bases:
                    PC.update()
                if 'GC' in bases:
                    GC.update()
                if 'BVC' in bases:
                    BVC.update()
                if "egoBVC" in bases:
                    egoBVC.update()
                if 'HDC' in bases:
                    HDC.update()

            # save firing rates for each neuron type
            if 'PC' in bases:
                PC_dict['firing_rates'][d, :, :-1] = np.array(PC.history['firingrate']).T
            if 'GC' in bases:
                GC_dict['firing_rates'][d, :, :-1] = np.array(GC.history['firingrate']).T
            if 'BVC' in bases:
                BVC_dict['firing_rates'][d, :, :-1] = np.array(BVC.history['firingrate']).T
            if 'egoBVC' in bases:
                egoBVC_dict['firing_rates'][d, :, :-1] = np.array(egoBVC.history['firingrate']).T
            if 'HDC' in bases:
                HDC_dict['firing_rates'][d, :, :-1] = np.array(HDC.history['firingrate']).T

            # clear history from agent
            for key in list(Ag.history.keys()):
                Ag.history[key] = []

            # clear history from neuron types (does not change weights or params of each cell)
            if 'PC' in bases:
                for key in list(PC.history.keys()):
                    PC.history[key] = []
            if 'GC' in bases:
                for key in list(GC.history.keys()):
                    GC.history[key] = []
            if 'BVC' in bases:
                for key in list(BVC.history.keys()):
                    BVC.history[key] = []
            if 'egoBVC' in bases:
                for key in list(egoBVC.history.keys()):
                    egoBVC.history[key] = []
            if 'HDC' in bases:
                for key in list(HDC.history.keys()):
                    HDC.history[key] = []
        if 'PC' in bases:
            joblib.dump(PC_dict, os.path.join(p_models, f'{animal}_PC_simulation'))
        if 'GC' in bases:
            joblib.dump(GC_dict, os.path.join(p_models, f'{animal}_GC_simulation'))
        if 'BVC' in bases:
            joblib.dump(BVC_dict, os.path.join(p_models, f'{animal}_BVC_simulation'))
        if 'egoBVC' in bases:
            joblib.dump(egoBVC_dict, os.path.join(p_models, f'{animal}_egoBVC_simulation'))
        if 'HDC' in bases:
            joblib.dump(HDC_dict, os.path.join(p_models, f'{animal}_HDC_simulation'))

        print(f'Animal {animal} basis set completed and saved!')
        if 'PC' in bases:
            del PC_dict
        if 'GC' in bases:
            del GC_dict
        if 'BVC' in bases:
            del BVC_dict
        if 'egoBVC' in bases:
            del egoBVC_dict
        if 'HDC' in bases:
            del HDC_dict


def load_bases(animal, p, bases=["GC", "BVC", "PC"]):
    """
    Load data from simulated basis set (or sets) for a target animal.
    :param animal: animal ID as string (e.g., "QLAK-CA1-08")
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param bases: list of bases to load into nested dictionary.
    :return: dictionary of simulated data for each basis set in bases
    """
    # load basis set from target cell type
    p_models = os.path.join(p, "results", "riab")
    basis_set = {}
    for basis in bases:
        basis_set[basis] = {}
        preload = joblib.load(os.path.join(p_models, f'{animal}_{basis}_simulation'))
        for key in list(preload.keys()):
            if key != 'animal':
                basis_set[basis][key] = preload[key][:]
        del preload
    print(f'{animal} set loaded')
    return basis_set


def get_model_maps(animals, p, feature_types=['BVC2PC'], n_bins=15, compute_rsm=False):
    """
    Create rate maps and optionally compute representational similarity matrix of rate maps with 3x3 partitioned
    environments from simulated firing of model features generated with riab.
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param feature_types: list of model feature types as strings (e.g., "GC2PC", "BVC2PC", etc...).
    :param n_bins: number of spatial bins for rate maps in x and y dimension
    :param compute_rsm: whether to compute representational similarity of model features from rate maps
    :return: None (rate maps and representational similarity matrix saved in p/results/riab folder).
    """
    # wrapper function for rate map generation from riab model simulation
    # simply adapted to dictionary specific fields of simulation data
    behav_dict = joblib.load(os.path.join(p, "data", 'behav_dict'))
    for animal in animals:
        p_models = os.path.join(p, "results", "riab")
        os.chdir(p_models)
        # define number of bins along x and y axis in rate maps
        for feature_type in feature_types:
            simulation = joblib.load(os.path.join(p_models, f'{animal}_{feature_type}_simulation'))
            simulation["firing_rates"]=np.nan_to_num(simulation["firing_rates"])
            # grab number of days and features to iterate across
            n_days, n_features = simulation['firing_rates'].shape[:-1]
            # initialize ratemaps (both smoothed and unsmoothed)
            maps = {'smoothed': np.zeros([n_bins, n_bins, n_features, n_days]) * np.nan,
                    'unsmoothed': np.zeros([n_bins, n_bins, n_features, n_days]) * np.nan}
            for d in tqdm(range(n_days), position=0, leave=True):
                maps['smoothed'][:, :, :, d] = get_rate_maps(simulation['position'][d].T,
                                                             simulation['firing_rates'][d].T,
                                                             n_bins=n_bins, filter_size=1.)[0].transpose(1, 2, 0)
                maps['unsmoothed'][:, :, :, d] = get_rate_maps(simulation['position'][d].T,
                                                               simulation['firing_rates'][d].T,
                                                               n_bins=n_bins, filter_size=False)[0].transpose(1, 2, 0)
            # clean up extraneous pixels in unsampled partitions
            maps = clean_rate_maps(maps, simulation['envs'])
            # save rate map data
            joblib.dump(maps, f'{animal}_{feature_type}_maps')
            if compute_rsm:
                models_maps = joblib.load(os.path.join(p_models, f'{animal}_{feature_type}_maps'))
                rsm_model, rsm_labels_model, cell_idx_model = get_cell_rsm_partitioned(models_maps)
                rsm_dict_model = {'RSM': rsm_model, 'd_labels': rsm_labels_model[:, 0], 'p_labels': rsm_labels_model[:, 1],
                                  'cell_idx': cell_idx_model, 'envs': behav_dict[animal]['envs']}
                joblib.dump(rsm_dict_model, f'{animal}_model_{feature_type}_rsm_partitioned_cellwise')
        del simulation


def get_solstad_pc(n_grids=50, gridscale=(0.28, 0.73), sigma=.12, threshold=False):
    """
    Model individual place cells as in Solstad et al. (2006) using riab toolbox.
    :param n_grids: number of grid cells to use as basis for each place cell
    :param gridscale: grid scale for grid cells. default is logarithmix as in Solstad et al. (2006).
    :param sigma: constant used for place cell calculation from grid inputs.
    :param threshold: minimum threshold for place cells to fire in model.
    :return: rate_map - rate map for individual place cell from riab; PC - riab place cell object (feed forward layer)
    """
    # Model place cells a la Solstad et al. (2006)
    Env = Environment(params={'aspect': 1, 'scale': .75, 'dimensionality': '2D'})
    Ag = Agent(Env)
    nPCs = 10
    for i in range(nPCs):
        shift_x, shift_y = np.random.uniform(0., 0.75, size=2)
        GC = GridCells(Ag, params={"n": n_grids,
                                   "gridscale": gridscale,
                                   "gridscale_distribution": "logarithmic",
                                   "phase_offset": (0., 0.),
                                   "min_fr": 0,
                                   "max_fr": 1,
                                   "name": "GridCells"})
        GC.phase_offsets = 2 * np.pi * np.array([shift_x, shift_y]) / GC.gridscales[:, None]
        PC = FeedForwardLayer(Ag, params={"n": 1,
                                          "features": GC,
                                          "input_layers": [GC],
                                          "min_fr": 0,
                                          "max_fr": 20,
                                          "activation_params": {"activation": "relu"}})
        # model GC2PC weights based on Solstad et al. (2006)
        weights = (np.exp(-(4/3)*np.pi**2*sigma**2/(GC.gridscales)**2)/GC.gridscales**2)
        PC.inputs['GridCells']['w'] = weights
        rate_map = PC.get_state(evaluate_at="all")
        rate_map = rate_map.reshape(int(np.sqrt(rate_map.shape[1])), int(np.sqrt(rate_map.shape[1])))
        if threshold:
            max_fr = np.amax(rate_map)
            rate_map -= max_fr * 0.8
            rate_map[rate_map < 0.] = 0.
    return rate_map, PC


def get_solstad_pc_population(n_pcs=500, n_grids=50, gridscale=(0.28, 0.73), sigma=0.12, threshold=False):
    """
    Generate a population of simulated place cells as in Solstad et al. (2006) using riab toolbox and get_solstad_pc
    :param n_pcs: number of place cells to simulate
    :param n_grids: number of grid cells to provide input for each place cell
    :param gridscale: grid scale for grid cells. default is logarithmix as in Solstad et al. (2006).
    :param sigma: constant used for place cell calculation from grid inputs.
    :param threshold: minimum threshold for place cells to fire in model.
    :return: array of n place cells
    """
    PCs = [None] * n_pcs
    for p in tqdm(range(n_pcs), desc="Simulating place cell rate maps a la Solstad et al. (2006)"):
        PCs[p], _ = get_solstad_pc(n_grids, gridscale, sigma, threshold)
    return np.array(PCs)


def get_gc2pc_maps(animals, p, pc_receptive_fields, threshold=True, n_pc=200, buffer=0.05, n_bins=15,
                   compute_rsm=False):
    """
    Generate rate maps from PC receptive fields calculated as in Solstad et al. (2006) following animal trajectories
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param pc_receptive_fields: place cell population receptive fields simulated with get_solstad_pc_population
    :param threshold: whether threshold was used to calculate receptive fields.
    :param n_pc: number of place cells simulated
    :param buffer: buffer size for rounding animal position in generation of rate maps
    :param n_bins: number of bins in x and y dimension for rate maps
    :param compute_rsm: whether to compute representational similarity matrix from simulated rate maps
    :return: None (PC model rate maps and representational similarity matrix saved in p/results/riab).
    """
    behav_dict = joblib.load(os.path.join(p, "data", "behav_dict"))
    for animal in animals:
        n_days = behav_dict[animal]["envs"].shape[0]
        len_recording = behav_dict[animal]["position"][0].shape[1]
        subsample_idx = np.random.choice(np.arange(pc_receptive_fields.shape[0]), n_pc, replace=False)
        pc_traces = np.zeros([n_days, n_pc, len_recording])
        pc_rate_maps = {"smoothed": np.zeros([n_days, n_pc, n_bins, n_bins]),
                        "unsmoothed": np.zeros([n_days, n_pc, n_bins, n_bins])}
        for d in tqdm(range(n_days), desc="Generating rate maps from receptive fields across days"):
            for t, (y, x) in enumerate(behav_dict[animal]["position"][d].T):
                pc_traces[d, :, t] = pc_receptive_fields[subsample_idx,
                                                         int(np.floor(x - buffer)), int(np.floor(y - buffer))]
            pc_rate_maps["smoothed"][d], _, _ = get_rate_maps(behav_dict[animal]["position"][d].T, pc_traces[d].T, n_bins)
            pc_rate_maps["unsmoothed"][d], _, _ = get_rate_maps(behav_dict[animal]["position"][d].T, pc_traces[d].T, n_bins,
                                                                filter_size=False)
        pc_rate_maps["smoothed"] = pc_rate_maps["smoothed"].transpose(2, 3, 1, 0)
        pc_rate_maps["unsmoothed"] = pc_rate_maps["unsmoothed"].transpose(2, 3, 1, 0)
        if threshold:
            joblib.dump(pc_rate_maps, os.path.join(p, "results", "riab", f"{animal}_GC2PC_th_maps"))

        else:
            joblib.dump(pc_rate_maps, os.path.join(p, "results", "riab", f"{animal}_GC2PC_maps"))

        if compute_rsm:
            rsm_model, rsm_labels_model, cell_idx_model = get_cell_rsm_partitioned(pc_rate_maps)
            rsm_dict_model = {'RSM': rsm_model, 'd_labels': rsm_labels_model[:, 0], 'p_labels': rsm_labels_model[:, 1],
                              'cell_idx': cell_idx_model, 'envs': behav_dict[animal]['envs']}
            if threshold:
                joblib.dump(rsm_dict_model, os.path.join(p, "results", "riab",
                                                         f'{animal}_model_GC2PC_th_rsm_partitioned_cellwise'))
            else:
                joblib.dump(rsm_dict_model, os.path.join(p, "results", "riab",
                                                         f'{animal}_model_GC2PC_rsm_partitioned_cellwise'))


def expand_3x3_rfdim(mat, part_size=25):
    """
    Expands 3x3 matrix of environments to represent true partition size of environment in cm
    :param mat: 3x3 matrix representation of environment generated with get_env_mat
    :param part_size: actual size in cm of environment partitions
    :return: matrix with expanded dimensions to reflect true location in cm for each partition
    """
    mat = mat[:, :, np.newaxis, np.newaxis].repeat(part_size, axis=-2).repeat(part_size, axis=-1)\
            .transpose(0, 2, 1, 3).reshape(part_size*3, part_size*3)
    return mat


def generate_boundary_fields(envs, env_size=75, thresh=0.8, plot_maps=False, binarize=True):
    """
    Generate four boudary fields as in Keinath et al. (2018) with riab to detect when animal is near boundary.
    :param envs: list of environments to create boundary fields for (e.g., "square", "o", "+", etc...).
    :param env_size: actual size of environment in cm
    :param thresh: binarize threshold for boundary field to consider animal is located at a boundary
    :param plot_maps: option to plot receptive fields maps for each environment
    :param binarize: option to binarize receptive fields.
    :return: receptive fields for each boundary as in Keinath et al. (2018)
    """
    boundary_fields = {}
    for env in tqdm(envs, desc="Creating boundary fields for all geometries", position=0, leave=True):
        Env = Environment(params={'aspect': 1, 'scale': .75, 'dimensionality': '2D'})
        Ag = Agent(Env)
        BC = BoundaryVectorCells(Ag, params={"n": 4,
                                 "reference_frame": "allocentric",
                                 "tuning_distance": np.zeros(4),
                                 "tuning_angle": np.array([0., 90., 180., 270.]),
                                 "min_fr": 0,
                                 "max_fr": 1,
                                 "name": "BoundaryVectorCells",
                                 "color": "C2"})

        deform_environment(Env, env)
        shape_sm = get_env_mat(env)
        shape_lg = expand_3x3_rfdim(shape_sm, env_size//3)
        shape_lg = shape_lg.astype(bool)
        BC_rf = BC.get_state(evaluate_at="all")
        BC_rf = BC_rf.reshape(4, env_size, env_size)
        if plot_maps:
            plt.figure()
            BC.plot_rate_map(chosen_neurons="4", method="groundtruth") # to plot receptive fields with RAIB
            plt.show()
        if binarize:
            BC_rf[BC_rf < thresh] = 0
            BC_rf[BC_rf > thresh] = 1
        BC_rf[:, ~shape_lg] = np.nan
        boundary_fields[env] = {"E": BC_rf[0], "N": BC_rf[1], "W": BC_rf[2], "S": BC_rf[3]}
    return boundary_fields


def generate_rf_shift_mask(envs):
    """
    Generate mask for shifting receptive fields in each environment geometry based on boundary conditions.
    :param envs: list of environments to create boundary fields for (e.g., "square", "o", "+", etc...).
    :return: mask for shifted receptive fields in each environment based on boundary conditions and direction
    """
    # set up square idx for each partition
    W_init = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).astype(float)
    S_init = np.rot90(W_init)
    E_init = np.rot90(S_init)
    N_init = np.rot90(E_init)
    # initialize dict for shifting mask
    rf_shift_mask = {}
    for env in tqdm(envs, desc="Creating shifted receptive fields for each boundary condition and environment",
                    position=0, leave=True):
        # index the partitions with deformation shape
        deformation = get_env_mat(env).astype(bool)
        # create west bound-tethering index
        W = deepcopy(W_init)
        W[~deformation] = np.nan
        for row in range(W.shape[0]):
            if np.any(np.isnan(W[row])):
                W[row] = np.nanmin(W_init[row])
        W[~deformation] = np.nan
        for x, y in np.argwhere(np.diff(np.hstack((np.zeros(3)[np.newaxis].T, W)), axis=1) == 0):
            W[x, y] += 1
        # create east bound-tethering index
        E = deepcopy(E_init)
        E[~deformation] = np.nan
        for row in range(E.shape[0]):
            if np.any(np.isnan(E[row])):
                E[row] = np.nanmin(E_init[row])
        E[~deformation] = np.nan
        for x, y in np.argwhere(np.diff(np.hstack((E, np.zeros(3)[np.newaxis].T)), axis=1) == 0):
            E[x, y] += 1
        # create south bound-tethering index
        N = deepcopy(N_init)
        N[~deformation] = np.nan
        for col in range(N.shape[1]):
            if np.any(np.isnan(N[:, col])):
                N[:, col] = np.nanmin(N_init[:, col])
        N[~deformation] = np.nan
        for x, y in np.argwhere(np.diff(np.vstack((np.zeros(3)[np.newaxis], N)), axis=0) == 0):
            N[x, y] += 1
        # create south bound-tethering index
        S = deepcopy(S_init)
        S[~deformation] = np.nan
        for col in range(S.shape[1]):
            if np.any(np.isnan(S[:, col])):
                S[:, col] = np.nanmin(S_init[:, col])
        S[~deformation] = np.nan
        for x, y in np.argwhere(np.diff(np.vstack((S, np.zeros(3)[np.newaxis])), axis=0) == 0):
            S[x, y] += 1
        # Flip N and S because "north" on matrix is actually south physically in matrix space (high row number)
        rf_shift_mask[env] = {"N": expand_3x3_rfdim(S),
                              "S": expand_3x3_rfdim(N),
                              "E": expand_3x3_rfdim(E),
                              "W": expand_3x3_rfdim(W)}
    return rf_shift_mask


def get_bound_teth_receptive_fields(receptive_fields, rf_shift_mask):
    """
    Generate boundary-tethered receptive fields for each environment based on boundary conditions and cardinal direction
    :param receptive_fields: original receptive fields to shift (e.g., PCs with Solstad et al. (2006) method)
    :param rf_shift_mask: mask for receptive field shifting in each environment
    :return: shifted receptive fields for each environment with given population (e.g., PCs with Solstad et al. (2006))
    """
    # use the rf_shift mask to create shifted receptive fields for each boundary
    pc_receptive_fields_shifted = {}
    for env in tqdm(list(rf_shift_mask.keys()), desc="Constructing receptive fields for environments",
                    position=0, leave=True):
        pc_receptive_fields_shifted[env] = {}
        for direction in list(rf_shift_mask[env].keys()):
            pc_receptive_fields_shifted[env][direction] = np.zeros_like(receptive_fields) * np.nan
            for f in range(receptive_fields.shape[0]):
                for part in range(1, 10):
                    part_mask = rf_shift_mask["square"][direction] == part
                    if np.any(~np.isnan(np.unique(rf_shift_mask[env][direction][part_mask])[0])):
                        def_part = int(np.unique(rf_shift_mask[env][direction][part_mask])[0])
                        def_part_mask = rf_shift_mask["square"][direction] == def_part
                        # want everything from def part mask in square to go to part_mask in deformation
                        pc_receptive_fields_shifted[env][direction][f][part_mask] = receptive_fields[f][def_part_mask]
    return pc_receptive_fields_shifted


def get_bound_teth_maps(animal, behav_dict, boundary_fields, pc_receptive_fields_shifted,
                        buffer=1e-5, filt_size=1.5):
    """
    Compute boundary-tethered rate maps as in Keinath et al. (2018), where cells dynamically shift following boundary
    approach in each geometry.
    :param animal: target animal ID as string (e.g., "QLAK-CA1-08")
    :param behav_dict: preloaded behavioral dictionary
    :param boundary_fields: receptive fields for each boundary along cardinal directions in each environment
    :param pc_receptive_fields_shifted: boundary-tethered receptive fields for place cell population
    :param buffer: buffer size to for rounding animal position to spatial bins
    :param filt_size: filter size for rate map smoothing
    :return: rate maps for boundary-tethered place cells
    """
    # get number of days
    n_days = behav_dict[animal]["envs"].shape[0]
    # initialize rate map dictionary
    maps = {"smoothed": [], "unsmoothed": []}
    for s in ["smoothed", "unsmoothed"]:
        # intialize tethered maps
        teth_maps = []
        for d in tqdm(np.arange(n_days), desc="Creating boundary-tethered rate maps", leave=True, position=0):
            # load position data and transpose from original orientation
            position = deepcopy(behav_dict[animal]["position"]).T[:, :, d]
            # need to invert y values to correct index subsequent receptive fields
            position[:, 1] *= -1
            position[:, 1] -= position[:, 1].min()
            # load environment / shape data
            env = behav_dict[animal]["envs"][d][0]
            teth_maps.append({})
            directions = sorted(boundary_fields[env].keys())
            # fields is a temp version of boundary fields to break it up into a sorted dict again
            fields_ = np.zeros_like(np.array(list((boundary_fields[env].values()))))
            for k, direction in enumerate(directions):
                fields_[k] = boundary_fields[env][direction]
            # boundary contact will indicate when animal is within N, S, E or W boundary field (shape = [N frames, 4])
            # first indicate whenever animal is within field
            boundary_contact = np.zeros([position.shape[0], len(directions)])
            for i, (x, y) in enumerate(position):
                boundary_contact[i] = fields_[:, int(np.floor(y - buffer)), int(np.floor(x - buffer))]
            # now update to maintain field to whatever was most recently contacted if animal is not near a wall
            for t, b in enumerate(boundary_contact):
                if not np.any(b):
                    boundary_contact[t] = boundary_contact[t-1]
            # finally change to a boolean
            boundary_contact = boundary_contact.astype(bool)
            # now initialize tethered maps for target day
            teth_maps[d] = {}
            # reload position data and use as floats for generating rate maps
            position_float = deepcopy(behav_dict[animal]["position"]).T[:, :, d]
            position_float[:, 1] *= -1
            position_float[:, 1] -= position_float[:, 1].min()
            for k, direction in enumerate(directions):
                # inialize temporary traces that will be filled with boundary-tethered rate map data
                temp_traces = np.zeros([position[boundary_contact[:, k]].shape[0],
                                        pc_receptive_fields_shifted[env][direction].shape[0]])
                for i, (x, y) in enumerate(position[boundary_contact[:, k]]):
                    # generate traces from boundary-tethered receptive fields
                    temp_traces[i, :] = pc_receptive_fields_shifted[env][direction][:,
                                        int(np.floor(y - buffer)), int(np.floor(x - buffer))]

                teth_maps[d][direction], _, _ = get_rate_maps(position_float[boundary_contact[:, k]], temp_traces,
                                                              filter_size=False)
            teth_maps[d] = np.nanmean(np.array(list(teth_maps[d].values())), 0)

        if s == "smoothed":
            nan_mask = np.isnan(np.array(teth_maps).transpose(2, 3, 1, 0))
            temp = np.nan_to_num(np.array(teth_maps).transpose(2, 3, 1, 0))
            temp = gaussian_filter1d(gaussian_filter1d(temp, sigma=filt_size, axis=0), sigma=filt_size, axis=1)
            temp[nan_mask] = np.nan
            maps[s] = np.fliplr(temp)
        else:
            maps[s] = np.fliplr(np.array(teth_maps).transpose(2, 3, 1, 0))
    return clean_rate_maps(maps, behav_dict[animal]["envs"])


def get_bt_gc2pc_maps(animals, p, n_pc=500, threshold=False, compute_rsm=False):
    """
    Wrapper function to compute and save boundary-tethered place cell maps with Keinath et al. (2018) method
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param n_pc: number of place cells to simulate for each animal
    :param threshold: whether inhibitory threshold was used to compute place cell receptive fields
    :param compute_rsm: whether to compute representational similarity matrix from rate maps
    :return: None (rate maps and representational similarity matrix saved in p/results/riab).
    """
    p_models = os.path.join(p, "results", "riab")
    behav_dict = joblib.load(os.path.join(p, "data", "behav_dict"))
    if threshold:
        receptive_fields = joblib.load(os.path.join(p_models, "solstad_gc2pc_receptive_fields_th"))
    else:
        receptive_fields = joblib.load(os.path.join(p_models, "solstad_gc2pc_receptive_fields"))
    for animal in animals:
        # generate sampling indices for receptive fields that were pre-computed from Solstad GC2PC model
        subsample_idx = np.random.choice(np.arange(receptive_fields.shape[0]), n_pc, replace=False)
        # draw receptive fields in square with sampling indices
        receptive_fields_animal = receptive_fields[subsample_idx]
        # generate boundary fields from square receptive fields template
        boundary_fields = generate_boundary_fields(np.unique(behav_dict[animal]['envs']))
        rf_shift_mask = generate_rf_shift_mask(np.unique(behav_dict[animal]['envs']))
        pc_receptive_fields_shifted = get_bound_teth_receptive_fields(receptive_fields_animal, rf_shift_mask)
        bt_gc2pc_maps = get_bound_teth_maps(animal, behav_dict, boundary_fields, pc_receptive_fields_shifted)
        if threshold:
            joblib.dump(bt_gc2pc_maps, os.path.join(p_models,
                                                    f'{animal}_bt_GC2PC_th_maps'))
        else:
            joblib.dump(bt_gc2pc_maps, os.path.join(p_models,
                                                    f'{animal}_bt_GC2PC_maps'))
        if compute_rsm:
            rsm_model, rsm_labels_model, cell_idx_model = get_cell_rsm_partitioned(bt_gc2pc_maps)
            rsm_dict_model = {'RSM': rsm_model, 'd_labels': rsm_labels_model[:, 0], 'p_labels': rsm_labels_model[:, 1],
                              'cell_idx': cell_idx_model, 'envs': behav_dict[animal]['envs']}
            if threshold:
                joblib.dump(rsm_dict_model, os.path.join(p_models,
                                                         f'{animal}_model_bt_GC2PC_th_rsm_partitioned_cellwise'))
            else:
                joblib.dump(rsm_dict_model, os.path.join(p_models,
                                                         f'{animal}_model_bt_GC2PC_rsm_partitioned_cellwise'))


def get_bvc2pc_maps(animal, p, nPCs = 500, compute_rsm=False):
    """
    Model place cells from boundary vector cell basis set as in Barry et al. (2006) and Grieves et al. (2018)
    :param animal: target animal ID as string (e.g., "QLAK-CA1-08")
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param nPCs: number of place cells to simulate for each animal
    :param compute_rsm: whether to compute representational similarity matrix from rate maps
    :return: None (rate maps and representational similarity matrix saved in p/results/riab).
    """
    p_models = os.path.join(p, "results", "riab")
    p_data = os.path.join(p, "data")
    start_time = time.time()
    print("Computing PC rate maps from model BVC rate maps")
    behav_dict = joblib.load(os.path.join(p_data, "behav_dict"))
    envs = behav_dict[animal]["envs"]
    os.chdir(p_models)
    doFitMaps = joblib.load(os.path.join(p_models, f"{animal}_BVC_maps"))["smoothed"]
    placeThreshold = 0.2
    # min and max number of BVC inputs to a place cell
    combAmt = (2, 16)
    # number of connections
    nCons = np.random.poisson(4, nPCs)
    # get number of BVCs
    nBVCs = doFitMaps.shape[2]
    placeMaps = np.zeros([15, 15, nPCs, envs.shape[0]]) * np.nan
    connections = [False] * nPCs
    # iterate through all sessions
    for si in tqdm(range(envs.shape[0]), desc="Fitting model cell wise across days", position=0, leave=True):
        for k in range(nPCs):
            # if first day of all recordings
            if si == 0:
                while ~np.any(connections[k]):
                    nCons = np.random.poisson(4, 1)[0]
                    if nCons < combAmt[0] or nCons > combAmt[1]:
                        continue
                    # generate random connections to all BVCs
                    inds = np.random.permutation(nBVCs)
                    # Then grab the connections constrained with number of inputs drawn from truncated poisson distrib.
                    m = doFitMaps[:, :, inds[:nCons], si]
                    # normalize the rate maps
                    m /= np.nanmax(m.reshape(-1, nCons), axis=0)
                    # calculate the geometric mean of BVC inputs (Grieves et al., 2018)
                    tmp = np.prod(m, axis=2) ** (1 / nCons)
                    # normalize and threshold BVC2PC rate map
                    tmp = tmp - np.nanmax(tmp) * placeThreshold
                    tmp[tmp < 0.] = 0.
                    # this is a scaling factor based on Grieves et al. (2018) to ensure Hz fall in usual range
                    tmp *= 500.
                    connections[k] = inds[:nCons]
            # use connections to simulate target cell on remaining days
            else:
                m = doFitMaps[:, :, connections[k], si]
                # normalize the rate maps
                m /= np.nanmax(m.reshape(-1, connections[k].shape[0]), axis=0)
                # calculate BVC rate maps same as above (Grieves et al., 2018)
                tmp = np.prod(m, axis=2) ** (1 / nCons)
                tmp = tmp - np.nanmax(tmp) * placeThreshold
                tmp[tmp < 0.] = 0.
                tmp *= 500.
            placeMaps[:, :, k, si] = tmp
    out_maps = {"smoothed": placeMaps}
    joblib.dump(out_maps, os.path.join(p_models, f"{animal}_BVC2PC_maps"))
    if compute_rsm:
        models_maps = joblib.load(os.path.join(p_models, f"{animal}_BVC2PC_maps"))
        rsm_model, rsm_labels_model, cell_idx_model = get_cell_rsm_partitioned(models_maps)
        rsm_dict_model = {'RSM': rsm_model, 'd_labels': rsm_labels_model[:, 0], 'p_labels': rsm_labels_model[:, 1],
                          'cell_idx': cell_idx_model, 'envs': envs}
        joblib.dump(rsm_dict_model, os.path.join(p_models,
                                                 f"{animal}_model_BVC2PC_rsm_partitioned_cellwise"))
    print("--- %s seconds ---" % (time.time() - start_time))


def simulate_basis2sf(animals, p, basis="BVC", sr_gamma=0.999, sr_alpha=(50./30.)*10**(-3),
                      norm_within_day=True, threshold=0.8, timestep=1, n_pretrain=3):
    """
    Compute successor features from desired basis set, temporal discount and learning rate parameters.
    :param animals: list of animal IDs as strings (e.g., ["QLAK-CA1-08", "QLAK-CA1-30", etc...]
    :param p: path of parent folder as string (e.g., "User/yourname/Desktop/georepca1")
    :param basis: model basis set to use as inputs to generate sucessor features (e.g., "PC" or "BVC").
    :param sr_gamma: temporal discount factor (0< and 1>).
    :param sr_alpha: learning rate
    :param norm_within_day: whether to normalize and threshold the features to max values within day
    :param threshold: proportion of max value to threshold features
    :param timestep: number of timesteps to step across for each update
    :param n_pretrain: number of square sessions to pre-train before computing features with behavior on remaining data
    :return: None (simulated sucessor features saved in p/results/riab).
    """
    # time the process
    start_time = time.time()
    p_models = os.path.join(p, "results", "riab")
    if glob(p_models) == []:
        os.mkdir(p_models)
    for animal in animals:
        # Load basis set for animal
        basis_set = load_bases(animal, p, bases=[basis])
        # grab position, environments, and firing rates from basis set
        basis_fr = np.nan_to_num(basis_set[basis]["firing_rates"])
        # get the number of days, number of bases (input cells), and steps
        n_days, n_bases, n_steps = basis_fr.shape
        # initialize sf simulation
        sf_simulation = {'animal': animal,
                         'envs': basis_set[basis]["envs"],
                         'position': basis_set[basis]["position"],
                         'firing_rates': np.zeros([n_days, n_bases, n_steps]) * np.nan}
        # initialize the successor feature matrix with ones
        M = np.ones([n_bases, n_bases])
        phi, n_phi = np.zeros([1, n_bases]), np.zeros([1, n_bases])
        print("Successor representation initialized")
        # set pre-training alpha to be the same as de Cothi paper
        pretrain_alpha = (50./30)*10**(-4)
        for _ in tqdm(range(n_pretrain), desc=f"Pre-training SF from {basis} inputs on first square day",
                      position=0, leave=True):
            for ti in range(n_steps - timestep):
                # phi is the bvc traces at current time step
                phi[0, :] = basis_fr[0, :, ti][np.newaxis]
                # n_phi is the bvc traces at next time step
                n_phi[0, :] = basis_fr[0, :, ti+timestep][np.newaxis]
                # update the successor feature representation with td learning rule
                M += pretrain_alpha * (phi.T + sr_gamma * (M @ n_phi.T) - (M @ phi.T)) @ phi
        # now that SR is pre-trained, step through all recorded sessions and compute SR with desired gamma and alpha
        across_session_max = np.zeros([n_bases, n_days])
        for si in tqdm(range(n_days), position=0, leave=True,
                           desc=f"Computing successor features from {basis} inputs across all sessions"):
            srTrace = np.zeros_like(basis_fr[si])
            for ti in range(n_steps - timestep):
                # phi is the bvc traces at current time step, and n_phi is the bvc traces at next time step
                phi[0, :] = basis_fr[si, :, ti][np.newaxis]
                n_phi[0, :] = basis_fr[si, :, ti + timestep][np.newaxis]
                # update the successor feature representation with td learning rule
                M += sr_alpha * (phi.T + sr_gamma * (M @ n_phi.T) - (M @ phi.T)) @ phi
                srTrace[:, ti] = np.nansum((M * phi), axis=1)
            sf_simulation['firing_rates'][si, :, :] = srTrace
            across_session_max[:, si] = np.amax(srTrace, axis=1)
        # now normalize each cell's firing to max firing either within or across all sessions
        if norm_within_day:
            for si in range(n_days):
                # normalize to max firing across days
                sf_simulation['firing_rates'][si] = (sf_simulation['firing_rates'][si].T / across_session_max[:, si]).T
                # only keep top 20%
                sf_simulation['firing_rates'][si] = (sf_simulation['firing_rates'][si] - threshold) / (1 - threshold)
                sf_simulation['firing_rates'][si][sf_simulation['firing_rates'][si] < 0] = 0.
        else:
            across_session_max = torch.amax(across_session_max, axis=1).cpu().numpy()
            for si in range(n_days):
                # normalize to max firing across days
                sf_simulation['firing_rates'][si] = (sf_simulation['firing_rates'][si].T / across_session_max).T
                # only keep top 20%
                sf_simulation['firing_rates'][si] = (sf_simulation['firing_rates'][si] - threshold) / (1-threshold)
                sf_simulation['firing_rates'][si][sf_simulation['firing_rates'][si] < 0] = 0.

        # clear the basis set for the animal
        del basis_set
        # save the sf model result
        joblib.dump(sf_simulation, os.path.join(p_models,
                    f'{animal}_{basis}2SF_{sr_gamma:.5f}gamma_{sr_alpha:.5f}alpha_simulation'))
        del sf_simulation
    print("--- %s seconds ---" % (time.time() - start_time))


########################################################################################################################
# MDS with fixes to non-metric method
def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
                   max_iter=300, verbose=0, eps=1e-3, random_state=None):
    """Computes multidimensional scaling using SMACOF algorithm
    Parameters
    ----------
    dissimilarities : ndarray, shape (n_samples, n_samples)
        Pairwise dissimilarities between the points. Must be symmetric.
    metric : boolean, optional, default: True
        Compute metric or nonmetric SMACOF algorithm.
    n_components : int, optional, default: 2
        Number of dimensions in which to immerse the dissimilarities. If an
        ``init`` array is provided, this option is overridden and the shape of
        ``init`` is used to determine the dimensionality of the embedding
        space.
    init : ndarray, shape (n_samples, n_components), optional, default: None
        Starting configuration of the embedding to initialize the algorithm. By
        default, the algorithm is initialized with a randomly chosen array.
    max_iter : int, optional, default: 300
        Maximum number of iterations of the SMACOF algorithm for a single run.
    verbose : int, optional, default: 0
        Level of verbosity.
    eps : float, optional, default: 1e-3
        Relative tolerance with respect to stress at which to declare
        convergence.
    random_state : int, RandomState instance, default=None
        Determines the random number generator used to initialize the centers.
        Pass an int for reproducible results across multiple function calls.
        See :term: `Glossary <random_state>`.
    Returns
    -------
    X : ndarray, shape (n_samples, n_components)
        Coordinates of the points in a ``n_components``-space.
    stress : float
        The final value of the stress (sum of squared distance of the
        disparities and the distances for all constrained points).
    n_iter : int
        The number of iterations corresponding to the best stress.
    """
    dissimilarities = check_symmetric(dissimilarities, raise_exception=True)

    n_samples = dissimilarities.shape[0]
    random_state = check_random_state(random_state)

    sim_flat = ((1 - np.tri(n_samples)) * dissimilarities).ravel()
    sim_flat_w = sim_flat[sim_flat != 0]
    if init is None:
        # Randomly choose initial configuration
        X = random_state.rand(n_samples * n_components)
        X = X.reshape((n_samples, n_components))
    else:
        # overrides the parameter p
        n_components = init.shape[1]
        if n_samples != init.shape[0]:
            raise ValueError("init matrix should be of shape (%d, %d)" %
                             (n_samples, n_components))
        X = init

    old_stress = None
    ir = IsotonicRegression()
    for it in range(max_iter):
        # Compute distance and monotonic regression
        dis = euclidean_distances(X)

        if metric:
            disparities = dissimilarities
        else:
            dis_flat = dis.ravel()
            # dissimilarities with 0 are considered as missing values
            dis_flat_w = dis_flat[sim_flat != 0]

            # Compute the disparities using a isotonic regression
            disparities = ir.fit_transform(dissimilarities.ravel(), dis.ravel()).reshape(n_samples, n_samples)
            # disparities = dis_flat.copy()
            # disparities[sim_flat != 0] = disparities_flat
            # sim = sim_flat.reshape((n_samples, n_samples))
            # disparities = disparities.reshape((n_samples, n_samples))
            # disparities[sim_flat.T!=0] = disprities[sim_flat!=0]
            disparities *= np.sqrt((n_samples * (n_samples - 1) / 2) /
                                   (disparities ** 2).sum())

        # Compute stress
        if metric:
            stress = np.sqrt(((dis.ravel() - disparities.ravel()) ** 2).sum() / disparities.ravel().sum())
        else:
            stress = np.sqrt(((dis.ravel() - disparities.ravel()) ** 2).sum() / dis.ravel().sum())

        # Update X using the Guttman transform
        dis[dis == 0] = 1e-5
        ratio = disparities / dis
        B = - ratio
        B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)
        X = 1. / n_samples * np.dot(B, X)

        dis = np.sqrt((X ** 2).sum(axis=1)).sum()
        if verbose >= 2:
            print('it: %d, stress %s' % (it, stress))
        if old_stress is not None:
            if (old_stress - stress / dis) < eps:
                if verbose:
                    print('breaking at iteration %d with stress %s' % (it,
                                                                       stress))
                break
        old_stress = stress / dis

    return X, stress, it + 1


@_deprecate_positional_args
def smacof(dissimilarities, *, metric=True, n_components=2, init=None,
           n_init=8, n_jobs=None, max_iter=300, verbose=0, eps=1e-3,
           random_state=None, return_n_iter=False):
    """Computes multidimensional scaling using the SMACOF algorithm.
    The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a
    multidimensional scaling algorithm which minimizes an objective function
    (the *stress*) using a majorization technique. Stress majorization, also
    known as the Guttman Transform, guarantees a monotone convergence of
    stress, and is more powerful than traditional techniques such as gradient
    descent.
    The SMACOF algorithm for metric MDS can summarized by the following steps:
    1. Set an initial start configuration, randomly or not.
    2. Compute the stress
    3. Compute the Guttman Transform
    4. Iterate 2 and 3 until convergence.
    The nonmetric algorithm adds a monotonic regression step before computing
    the stress.
    Parameters
    ----------
    dissimilarities : ndarray, shape (n_samples, n_samples)
        Pairwise dissimilarities between the points. Must be symmetric.
    metric : boolean, optional, default: True
        Compute metric or nonmetric SMACOF algorithm.
    n_components : int, optional, default: 2
        Number of dimensions in which to immerse the dissimilarities. If an
        ``init`` array is provided, this option is overridden and the shape of
        ``init`` is used to determine the dimensionality of the embedding
        space.
    init : ndarray, shape (n_samples, n_components), optional, default: None
        Starting configuration of the embedding to initialize the algorithm. By
        default, the algorithm is initialized with a randomly chosen array.
    n_init : int, optional, default: 8
        Number of times the SMACOF algorithm will be run with different
        initializations. The final results will be the best output of the runs,
        determined by the run with the smallest final stress. If ``init`` is
        provided, this option is overridden and a single run is performed.
    n_jobs : int or None, optional (default=None)
        The number of jobs to use for the computation. If multiple
        initializations are used (``n_init``), each run of the algorithm is
        computed in parallel.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.
    max_iter : int, optional, default: 300
        Maximum number of iterations of the SMACOF algorithm for a single run.
    verbose : int, optional, default: 0
        Level of verbosity.
    eps : float, optional, default: 1e-3
        Relative tolerance with respect to stress at which to declare
        convergence.
    random_state : int, RandomState instance, default=None
        Determines the random number generator used to initialize the centers.
        Pass an int for reproducible results across multiple function calls.
        See :term: `Glossary <random_state>`.
    return_n_iter : bool, optional, default: False
        Whether or not to return the number of iterations.
    Returns
    -------
    X : ndarray, shape (n_samples, n_components)
        Coordinates of the points in a ``n_components``-space.
    stress : float
        The final value of the stress (sum of squared distance of the
        disparities and the distances for all constrained points).
    n_iter : int
        The number of iterations corresponding to the best stress. Returned
        only if ``return_n_iter`` is set to ``True``.
    Notes
    -----
    "Modern Multidimensional Scaling - Theory and Applications" Borg, I.;
    Groenen P. Springer Series in Statistics (1997)
    "Nonmetric multidimensional scaling: a numerical method" Kruskal, J.
    Psychometrika, 29 (1964)
    "Multidimensional scaling by optimizing goodness of fit to a nonmetric
    hypothesis" Kruskal, J. Psychometrika, 29, (1964)
    """

    dissimilarities = check_array(dissimilarities)
    random_state = check_random_state(random_state)

    if hasattr(init, '__array__'):
        init = np.asarray(init).copy()
        if not n_init == 1:
            warnings.warn(
                'Explicit initial positions passed: '
                'performing only one init of the MDS instead of %d'
                % n_init)
            n_init = 1

    best_pos, best_stress = None, None

    if effective_n_jobs(n_jobs) == 1:
        for it in range(n_init):
            pos, stress, n_iter_ = _smacof_single(
                dissimilarities, metric=metric,
                n_components=n_components, init=init,
                max_iter=max_iter, verbose=verbose,
                eps=eps, random_state=random_state)
            if best_stress is None or stress < best_stress:
                best_stress = stress
                best_pos = pos.copy()
                best_iter = n_iter_
    else:
        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
        results = Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))(
            delayed(_smacof_single)(
                dissimilarities, metric=metric, n_components=n_components,
                init=init, max_iter=max_iter, verbose=verbose, eps=eps,
                random_state=seed)
            for seed in seeds)
        positions, stress, n_iters = zip(*results)
        best = np.argmin(stress)
        best_stress = stress[best]
        best_pos = positions[best]
        best_iter = n_iters[best]

    if return_n_iter:
        return best_pos, best_stress, best_iter
    else:
        return best_pos, best_stress


class MDS(BaseEstimator):
    """Multidimensional scaling
    Read more in the :ref:`User Guide <multidimensional_scaling>`.
    Parameters
    ----------
    n_components : int, optional, default: 2
        Number of dimensions in which to immerse the dissimilarities.
    metric : boolean, optional, default: True
        If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.
    n_init : int, optional, default: 4
        Number of times the SMACOF algorithm will be run with different
        initializations. The final results will be the best output of the runs,
        determined by the run with the smallest final stress.
    max_iter : int, optional, default: 300
        Maximum number of iterations of the SMACOF algorithm for a single run.
    verbose : int, optional, default: 0
        Level of verbosity.
    eps : float, optional, default: 1e-3
        Relative tolerance with respect to stress at which to declare
        convergence.
    n_jobs : int or None, optional (default=None)
        The number of jobs to use for the computation. If multiple
        initializations are used (``n_init``), each run of the algorithm is
        computed in parallel.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
        for more details.
    random_state : int, RandomState instance, default=None
        Determines the random number generator used to initialize the centers.
        Pass an int for reproducible results across multiple function calls.
        See :term: `Glossary <random_state>`.
    dissimilarity : 'euclidean' | 'precomputed', optional, default: 'euclidean'
        Dissimilarity measure to use:
        - 'euclidean':
            Pairwise Euclidean distances between points in the dataset.
        - 'precomputed':
            Pre-computed dissimilarities are passed directly to ``fit`` and
            ``fit_transform``.
    Attributes
    ----------
    embedding_ : array-like, shape (n_samples, n_components)
        Stores the position of the dataset in the embedding space.
    stress_ : float
        The final value of the stress (sum of squared distance of the
        disparities and the distances for all constrained points).
    Examples
    --------
    >>> from sklearn.datasets import load_digits
    >>> from sklearn.manifold import MDS
    >>> X, _ = load_digits(return_X_y=True)
    >>> X.shape
    (1797, 64)
    >>> embedding = MDS(n_components=2)
    >>> X_transformed = embedding.fit_transform(X[:100])
    >>> X_transformed.shape
    (100, 2)
    References
    ----------
    "Modern Multidimensional Scaling - Theory and Applications" Borg, I.;
    Groenen P. Springer Series in Statistics (1997)
    "Nonmetric multidimensional scaling: a numerical method" Kruskal, J.
    Psychometrika, 29 (1964)
    "Multidimensional scaling by optimizing goodness of fit to a nonmetric
    hypothesis" Kruskal, J. Psychometrika, 29, (1964)
    """

    @_deprecate_positional_args
    def __init__(self, n_components=2, *, metric=True, n_init=4,
                 max_iter=300, verbose=0, eps=1e-3, n_jobs=None,
                 random_state=None, dissimilarity="euclidean"):
        self.n_components = n_components
        self.dissimilarity = dissimilarity
        self.metric = metric
        self.n_init = n_init
        self.max_iter = max_iter
        self.eps = eps
        self.verbose = verbose
        self.n_jobs = n_jobs
        self.random_state = random_state

    @property
    def _pairwise(self):
        return self.kernel == "precomputed"

    def fit(self, X, y=None, init=None):
        """
        Computes the position of the points in the embedding space
        Parameters
        ----------
        X : array, shape (n_samples, n_features) or (n_samples, n_samples)
            Input data. If ``dissimilarity=='precomputed'``, the input should
            be the dissimilarity matrix.
        y : Ignored
        init : ndarray, shape (n_samples,), optional, default: None
            Starting configuration of the embedding to initialize the SMACOF
            algorithm. By default, the algorithm is initialized with a randomly
            chosen array.
        """
        self.fit_transform(X, init=init)
        return self

    def fit_transform(self, X, y=None, init=None):
        """
        Fit the data from X, and returns the embedded coordinates
        Parameters
        ----------
        X : array, shape (n_samples, n_features) or (n_samples, n_samples)
            Input data. If ``dissimilarity=='precomputed'``, the input should
            be the dissimilarity matrix.
        y : Ignored
        init : ndarray, shape (n_samples,), optional, default: None
            Starting configuration of the embedding to initialize the SMACOF
            algorithm. By default, the algorithm is initialized with a randomly
            chosen array.
        """
        X = self._validate_data(X)
        if X.shape[0] == X.shape[1] and self.dissimilarity != "precomputed":
            warnings.warn("The MDS API has changed. ``fit`` now constructs an"
                          " dissimilarity matrix from data. To use a custom "
                          "dissimilarity matrix, set "
                          "``dissimilarity='precomputed'``.")

        if self.dissimilarity == "precomputed":
            self.dissimilarity_matrix_ = X
        elif self.dissimilarity == "euclidean":
            self.dissimilarity_matrix_ = euclidean_distances(X)
        else:
            raise ValueError("Proximity must be 'precomputed' or 'euclidean'."
                             " Got %s instead" % str(self.dissimilarity))

        self.embedding_, self.stress_, self.n_iter_ = smacof(
            self.dissimilarity_matrix_, metric=self.metric,
            n_components=self.n_components, init=init, n_init=self.n_init,
            n_jobs=self.n_jobs, max_iter=self.max_iter, verbose=self.verbose,
            eps=self.eps, random_state=self.random_state,
            return_n_iter=True)

        return self.embedding_
